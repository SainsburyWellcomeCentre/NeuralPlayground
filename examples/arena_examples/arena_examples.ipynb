{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arena Class Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook, we provide examples to create arenas so an agent can interact with.\n",
    "Each of these arenas has common methods that resemble Open AI gym structure (soon we will built our classes on top of Open AI gym environments).\n",
    "The package has methods to build 2D arenas with arbitrary shapes described by \"walls\" that can be easely change for each arena. There are also arenas that resemble sampling rate and dimensions of specific experiments (see \"Experiment Class Example\" notebook), one using these arenas, the experimental data (animal trajectory and neural recording) is loaded from its respective dataset, and can be used to compare results againts the artificial agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's initialize a random agent to evaluate on each envirnoment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralplayground.agents import RandomAgent, LevyFlightAgent\n",
    "# Random agent generates a brownian motion. Levy flight is still experimental.\n",
    "agent = LevyFlightAgent(step_size=0.8, scale=2.0, loc=0.0, beta=1.0, alpha=0.5, max_action_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D arenas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These arenas are maleable and can take arbitrary shapes based on the configuration its walls. Here we start by a default square room."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralplayground.arenas import Simple2D, ConnectedRooms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the arena/environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step_size = 0.1 #seg\n",
    "agent_step_size = 3\n",
    "\n",
    "# Init environment\n",
    "env = Simple2D(time_step_size = time_step_size,\n",
    "               agent_step_size = agent_step_size,\n",
    "               arena_x_limits=(-100, 100), \n",
    "               arena_y_limits=(-100, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all arenas, you can run the same \"training loop\" where the agents recieves and observation from the environment (in this case the position), then chose an action based on the observation, and observe the outcome from the environment given the action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 5000#50000\n",
    "\n",
    "# Initialize environment\n",
    "obs, state = env.reset()\n",
    "for i in range(n_steps):\n",
    "    # Observe to choose an action\n",
    "    action = agent.act(obs)\n",
    "    # Run environment for given action\n",
    "    obs, state, reward = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The position history of the agent is saved within the environment, you can plot the trajectory using the method ```plot_trajectory```, or have access to the positions by asking for the ```history``` attribute of the environment class (```h = env.history```). Using ```env.reset()```, will erase the history of interaction of the agent with the enviornment, and reposition the agent randomly, or can be located in a specific place using the ```custom_state``` argument in the reset function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = env.plot_trajectory()\n",
    "fontsize = 18\n",
    "ax.grid()\n",
    "# ax.legend(fontsize=fontsize, loc=\"upper left\")\n",
    "ax.set_xlabel(\"width\", fontsize=fontsize)\n",
    "ax.set_ylabel(\"depth\", fontsize=fontsize)\n",
    "# plt.savefig(\"two_rooms.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some other environments. There is a ```Connected room``` class (hopefully will include neural recordings from [this paper](https://www.sciencedirect.com/science/article/pii/S0893608019302631) soon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step_size = 0.1 #seg\n",
    "agent_step_size = 3\n",
    "\n",
    "# Init environment\n",
    "env = ConnectedRooms(time_step_size = time_step_size,\n",
    "                     agent_step_size = agent_step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 3000#30000\n",
    "\n",
    "# Initialize environment\n",
    "obs, state = env.reset()\n",
    "for i in range(n_steps):\n",
    "    # Observe to choose an action\n",
    "    action = agent.act(obs)\n",
    "    # Run environment for given action\n",
    "    obs, state, reward = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = env.plot_trajectory()\n",
    "fontsize = 18\n",
    "ax.grid()\n",
    "# ax.legend(fontsize=fontsize, loc=\"upper left\")\n",
    "ax.set_xlabel(\"width\", fontsize=fontsize)\n",
    "ax.set_ylabel(\"depth\", fontsize=fontsize)\n",
    "plt.savefig(\"two_rooms.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe you didn't noticed this detail, but the inner walls in the plot are blue! and these are the walls that are maleable and listed in the attribute called ```custom_walls```, we can easely change the walls by overriding a method from these arenas called ```_create_custom_walls```. In this method, the ```custom_walls``` attribute is define to built the blue walls within the limits of the arena (which are the red walls). So, for example, the ```custom_walls``` of the ```ConnectedRooms``` class are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.custom_walls, env.custom_walls[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element in the ```custom_walls``` list is a 2x2 matrix where the first row is the (x, y) coordinate of the first limit of the wall, and the second row is the (x, y) coordinate of the second limit of the wall. Just by modifying this attribute (or overriding ```_create_custom_walls```) we can change the shape of the arena. Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoArms(ConnectedRooms):\n",
    "    \n",
    "    def _create_custom_walls(self):  \n",
    "        \n",
    "        # this method is called every time an arena from this class is instantiated\n",
    "        \n",
    "        self.custom_walls = []  # Create the list with custom_walls (needs to be created)\n",
    "        self.custom_walls.append(np.array([[-25, 0], [-25, -self.singleroom_ysize]]))  # singleroom_ysize are attributes from the connected room class\n",
    "        self.custom_walls.append(np.array([[25, 0], [25, -self.singleroom_ysize]]))    \n",
    "        self.custom_walls.append(np.array([[-self.singleroom_xsize, 0], [-25, 0]]))\n",
    "        self.custom_walls.append(np.array([[self.singleroom_xsize, 0], [25, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just changed the custom_wall list that is built when instantiate the class, let's see how it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"two_arms\"\n",
    "time_step_size = 0.1 #seg\n",
    "agent_step_size = 5\n",
    "\n",
    "# Init environment\n",
    "env = TwoArms(environment_name=env_name,\n",
    "              time_step_size = time_step_size,\n",
    "              agent_step_size = agent_step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 3000#30000\n",
    "\n",
    "# Initialize environment\n",
    "obs, state = env.reset(custom_state=[0,0])\n",
    "for i in range(n_steps):\n",
    "    # Observe to choose an action\n",
    "    action = agent.act(obs)\n",
    "    # Run environment for given action\n",
    "    obs, state, reward = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = env.plot_trajectory()\n",
    "fontsize = 16\n",
    "ax.grid()\n",
    "# ax.legend(fontsize=fontsize, loc=\"upper left\")\n",
    "ax.set_xlabel(\"width\", fontsize=fontsize)\n",
    "ax.set_ylabel(\"depth\", fontsize=fontsize)\n",
    "# plt.savefig(\"two_arms.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also define functions that generate more comples walls, for example, we defined a ```create_circular_wall``` (check in [utils](https://github.com/ClementineDomine/EHC_model_comparison/blob/main/neuralplayground/utils.py)) that gives you a list of stright walls that simulate a circle. So let's create a circular hall arena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralplayground.arenas import Simple2D\n",
    "from neuralplayground.utils import create_circular_wall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircularArena(Simple2D):\n",
    "    \n",
    "    def _create_custom_walls(self):\n",
    "        self.custom_walls = create_circular_wall(center=np.array([0, 0]), radius=50)  # One inner circle\n",
    "        self.custom_walls += create_circular_wall(center=np.array([0, 0]), radius=75)  # One outer circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step_size = 0.1 #seg\n",
    "agent_step_size = 5\n",
    "arena_x_limits = np.array((-100, 100))\n",
    "arena_y_limits = np.array((-100, 100))\n",
    "\n",
    "# Init environment\n",
    "env = CircularArena(time_step_size = time_step_size,\n",
    "                    agent_step_size = agent_step_size,\n",
    "                    arena_x_limits=arena_x_limits,\n",
    "                    arena_y_limits=arena_y_limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 3000#30000\n",
    "\n",
    "# We start the agent within the circular hall, if we don't do this, the agent might start from outside the hall\n",
    "# Try it, by removing the custom_state argument\n",
    "obs, state = env.reset(custom_state=[-70,0])\n",
    "# obs, state = env.reset(custom_state = [-90, 0])\n",
    "for i in range(n_steps):\n",
    "    # Observe to choose an action\n",
    "    action = agent.act(obs)\n",
    "    # Run environment for given action\n",
    "    obs, state, reward = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = env.plot_trajectory()\n",
    "fontsize = 16\n",
    "ax.grid()\n",
    "# ax.legend(fontsize=fontsize, loc=\"upper left\")\n",
    "ax.set_xlabel(\"width\", fontsize=fontsize)\n",
    "ax.set_ylabel(\"depth\", fontsize=fontsize)\n",
    "plt.savefig(\"circle.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arenas with experimental data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These arenas are pre-configured to follow the dimensions and sampling rate of a real experiment. When initializing these arenas, the recorded data from the corresponding experiment will be automatically loaded using the Experimental Data Class (see [these examples](https://github.com/ClementineDomine/NeuralPlayground/blob/main/examples/experimental_examples/experimental_data_examples.ipynb)). The idea is to create artificial environment with a structure that resembles the experiment, to then compare with experimental recordings. For now, these classes are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralplayground.arenas import Sargolini2006\n",
    "from neuralplayground.arenas import Hafting2008\n",
    "from neuralplayground.arenas import Wernle2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these classes corresponds to a different Experimental paper. Let's use Sargolini2006 as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Sargolini2006(environment_name = env_name,\n",
    "                    time_step_size = time_step_size,\n",
    "                    agent_step_size = agent_step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.experiment\n",
    "# run the command below for docstring display\n",
    "# help(env.experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the methods from the Experiment class are included in these specific arena classes, these methods are ```show_data()```, ```set_animal_data()```, ```plot_recording_tetr()```, ```plot_recorded_trajectory()```. These methods works analogous to the ones available in the [Experiment classes](https://github.com/ClementineDomine/NeuralPlayground/blob/main/examples/experimental_examples/experimental_data_examples.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_list = env.show_data()  # env.data has an object of the experiment class with Sargolini et al data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.plot_recorded_trajectory(recording_index=3, plot_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the same interaction loop with the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 3000#30000\n",
    "\n",
    "# The dimensions of the arena are the same as the one from the experiment (in cm)\n",
    "obs, state = env.reset()\n",
    "for i in range(n_steps):\n",
    "    # Observe to choose an action\n",
    "    action = agent.act(obs)\n",
    "    # Run environment for given action\n",
    "    obs, state, reward = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = env.plot_trajectory()\n",
    "fontsize = 16\n",
    "ax.grid()\n",
    "# ax.legend(fontsize=fontsize, loc=\"upper left\")\n",
    "ax.set_xlabel(\"width\", fontsize=fontsize)\n",
    "ax.set_ylabel(\"depth\", fontsize=fontsize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic arenas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these environment might change through time, like in [Wernle et al 2018](https://www.nature.com/articles/s41593-017-0036-6). In this experiment, mice would explore an arena with a wall in between that doesn't allow the animal to cross to the other side. After a few minutes, the mouse would be moved to the other side of the arena, still with the wall. Then, the wall is removed, and the mouse is allowed to explore both sides of the arena. The idea was to study how grid fields merge after removing the wall in between. Let's run this experiment using a built in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step_size = 0.2\n",
    "agent_step_size = 3\n",
    "merging_time = 20  # Time in minutes to remove \n",
    "switch_time = 10 # Time in minutes to move the mouse to the other side of the arena\n",
    "n_steps = ((merging_time + switch_time)*60) / time_step_size\n",
    "\n",
    "env = Wernle2018(merge_time=merging_time,\n",
    "                 switch_time=switch_time,\n",
    "                 time_step_size=time_step_size,\n",
    "                 agent_step_size=agent_step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we have acess to the data from this experiment. The data is loaded automatically and define as an attribute of the arena class called ```data```. Below, we plot the grid fields before and after merging time recorded in the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.time_step_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the rate map for different grid cells before an after the merging of the wall using a method from the experiment class initialized in the arena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.experiment.plot_merging_comparison(session_index=(125, 126, 127))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following loop, we run the arena-agent interaction that works in the same way as the previous ones. The switching of rooms and removing of the mid wall is a process written in the class, so we don't need to care about coding this part. If you want to modify or built your own dynamic arena, it is just a matter of change the ```custom_wall``` list, you can use [this same class](https://github.com/ClementineDomine/EHC_model_comparison/blob/main/neuralplayground/arenas/wernle_2018.py) as an example. We will just add an \"if\" statement to plot the exploration of the agent before and after switching rooms but **before merging**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(n_steps))\n",
    "print(int((merging_time*60)/time_step_size)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, state = env.reset()\n",
    "\n",
    "for j in range(round(n_steps)):\n",
    "    # Observe to choose an action\n",
    "    action = agent.act(obs)\n",
    "    # Run environment for given action\n",
    "    obs, state, reward = env.step(action)\n",
    "    # We plot the trajectory of the agent just right before the merge\n",
    "    # then we keep running the loop\n",
    "    if j == int((merging_time*60)/time_step_size)-1:\n",
    "        ax = env.plot_trajectory()\n",
    "        ax.set_xlabel(\"width\", fontsize=fontsize)\n",
    "        ax.set_ylabel(\"depth\", fontsize=fontsize)\n",
    "        plt.savefig(\"pre_merge.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot the exploration of the agent after merging (removing the wall in the middle) by first getting the whole history of the interaction from the arena, saved in the attribute ```env.history```, then taking the history that is just exploring the merged room. We can re-use the ```plot_trajectory``` method to show any history, so we take the time points we are intereseted in, then feed it to the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we take the section of interaction history for the period after the merge\n",
    "merged_history = env.history[int((merging_time*60)/time_step_size):]\n",
    "# Note that the wall is already removed from the arena structure\n",
    "ax = env.plot_trajectory(history_data=merged_history)\n",
    "ax.set_xlabel(\"width\", fontsize=fontsize)\n",
    "ax.set_ylabel(\"depth\", fontsize=fontsize)\n",
    "\n",
    "plt.savefig(\"after_merge.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These are some of the features we implemented in this specific class, and we plan to include much more with time, including more experimental settings, 2d visual stimuli, easy customization for plots and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
