{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://githubtocolab.com/SainsburyWellcomeCentre/NeuralPlayground/examples/colab_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Lq6vy5PwRV5"
   },
   "source": [
    "# NeuralPlayground Introduction\n",
    "## If you using colab, install requirements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rd62T6bZwOZN"
   },
   "source": [
    "pip install neuralplayground"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From here if you installed the package and re-started the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import neuralplayground\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class gives access to open-source experimental data (neural recording, behaviours, etc.) through various plotting\n",
    "functions and visualisations of experimental measurements. Each data set is organised in recordings session with an\n",
    "attributed recording number (rec index), given in a list at the initialisation of the class. It is possible to plot a\n",
    "selected tetrode (select the index in the list) recording ```plot_recording_tetr()```, the trajectory recording\n",
    "within the arena ```plot_trajectory()``` and get access to the experimental details call ```show_keys()```.\n",
    "For further explanation of the datasets, check the [notebook examples](https://github.com/ClementineDomine/NeuralPlayground/blob/main/examples/experimental_examples/experimental_data_examples.ipynb) using this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "from neuralplayground.experiments import Sargolini2006Data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "sargolini_data = Sargolini2006Data(verbose=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "rate_map, x_bin, y_bin = sargolini_data.plot_recording_tetr(recording_index=4, tetrode_id = \"T6C3\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "x, y, time_steps = sargolini_data.plot_trajectory(recording_index=4, plot_every=5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arenas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arena provides an environment in which an agent can explore and potentially learn\n",
    "over time, reproducing aspects of the physical layout of an experimental paradigm in which behavioral and neural data\n",
    "were collected. Any two-dimensional discrete and continuous Arenas can be built\n",
    "using walls as construction units. This allows complex experimental architectures such as connected rooms, T-mazes\n",
    "or cycles to be added. Dynamical arenas, such as the merging room experiment in [Wernle et\n",
    "al. (2018)](https://github.com/ClementineDomine/NeuralPlayground/blob/main/neuralplayground/arenas/wernle_2018.py)\n",
    "are also be implemented. Each specific environment implemented to resemble an experimental setting should be\n",
    "created as a subclass of the main environment class. The Environment can be initialised with data from real-life\n",
    "experiments. We will work toward improving each of the Environments through the projects, adding experimental specifications,\n",
    "richer perceptual inputs and flexibility to analyze and run simulations.\n",
    "\n",
    "Check the [arena examples notebook](https://github.com/ClementineDomine/NeuralPlayground/blob/main/examples/arena_examples/arena_examples.ipynb) for further explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "from neuralplayground.arenas import Simple2D, ConnectedRooms"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "from neuralplayground.agents import RandomAgent, LevyFlightAgent\n",
    "# Random agent generates a brownian motion. Levy flight is still experimental.\n",
    "agent = LevyFlightAgent(step_size=0.8, scale=2.0, loc=0.0, beta=1.0, alpha=0.5, max_action_size=100)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "time_step_size = 0.1 #seg\n",
    "agent_step_size = 3\n",
    "\n",
    "# Init environment\n",
    "env = Simple2D(time_step_size = time_step_size,\n",
    "               agent_step_size = agent_step_size,\n",
    "               arena_x_limits=(-100, 100), \n",
    "               arena_y_limits=(-100, 100))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "n_steps = 5000#50000\n",
    "\n",
    "# Initialize environment\n",
    "obs, state = env.reset()\n",
    "for i in range(n_steps):\n",
    "    # Observe to choose an action\n",
    "    action = agent.act(obs)\n",
    "    # Run environment for given action\n",
    "    obs, state, reward = env.step(action)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "ax = env.plot_trajectory()\n",
    "fontsize = 18\n",
    "ax.grid()\n",
    "# ax.legend(fontsize=fontsize, loc=\"upper left\")\n",
    "ax.set_xlabel(\"width\", fontsize=fontsize)\n",
    "ax.set_ylabel(\"depth\", fontsize=fontsize)\n",
    "# plt.savefig(\"two_rooms.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class includes a set of functions that control the ways intelligent systems interact\n",
    "with their surroundings (i.e., the environment). An agent receives observations from the environment (reward, visual cues, etc.) and uses these to take an action which in turn will update both its state and the state of the environment, generating new observations. More generally, the Agent can be thought of as an animal performing the task in the simulated experiment. All agent types will be given a set of abilities that are summarised in the agentâ€™s main class. Each different model developed can be easily implemented as a subclass of the main one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "from neuralplayground.agents import Weber2018"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "# Parameters for a square environment\n",
    "room_width = [-10,10]\n",
    "room_depth = [-10,10]\n",
    "env_name = \"env_example\"\n",
    "time_step_size = 1\n",
    "agent_step_size = 0.5\n",
    "\n",
    "# Init environment\n",
    "envsimple = Simple2D(arena_x_limits = room_width,\n",
    "                     arena_y_limits = room_depth,\n",
    "                     time_step_size = time_step_size,\n",
    "                     agent_step_size = agent_step_size)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "exc_eta = 2e-4\n",
    "inh_eta = 8e-4\n",
    "model_name = \"model_example\"\n",
    "sigma_exc = np.array([0.05, 0.05])\n",
    "sigma_inh = np.array([0.1, 0.1])\n",
    "Ne = 4900\n",
    "Ni = 1225\n",
    "Nef = 1\n",
    "Nif = 1\n",
    "alpha_i = 1\n",
    "alpha_e = 1\n",
    "we_init = 1.0\n",
    "wi_init = 1.5\n",
    "agent_step_size = 0.1\n",
    "roh = 1\n",
    "agent = Weber2018(model_name=model_name, exc_eta=exc_eta, inh_eta=inh_eta, sigma_exc=sigma_exc,\n",
    "                  sigma_inh=sigma_inh, Ne=Ne, Ni=Ni, agent_step_size=agent_step_size, ro=roh,\n",
    "                  Nef=Nef, Nif=Nif, room_width=envsimple.room_width, room_depth=envsimple.room_depth,\n",
    "                  alpha_i=alpha_i, alpha_e=alpha_e, we_init=we_init, wi_init=wi_init)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "plot_every = 25000  # 100000\n",
    "total_iters = 0\n",
    "n_steps = 100000 # 400000 For cleaner results\n",
    "obs, state = envsimple.reset()\n",
    "for i in tqdm(range(n_steps)):\n",
    "    # Observe to choose an action, the first to numbers in the observation are the xy position of the agent\n",
    "    obs = obs[:2]\n",
    "    action = agent.act(obs)\n",
    "    # rate = agent.update()\n",
    "    agent.update()\n",
    "    # Run environment for given action\n",
    "    obs, state, reward = envsimple.step(action, normalize_step=True)\n",
    "    total_iters += 1\n",
    "    if i % plot_every == 0 or i == n_steps - 1:\n",
    "        agent.plot_rate_map()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we show some examples of comparisons between simulations of theoretical models and experimental data, being this the main goal of our software in the long term. \n",
    "\n",
    "This aspect of the NeuralPlayground (NPG) software is still under development. With time, we want to implement tools exclusively made to compare simulated and experimental data, such as a GUI to quickly navigate through the results, add more metrics to measure the performance of agents or similarity between neural representation.\n",
    "\n",
    "We compare two agents in the same environement and compare their representations. The runs have been previoulsy run using the simulation manager, the output was saved to the gin repositiory for ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "from neuralplayground.saved_models import fetch_model_path\n",
    "import pandas as pd\n",
    "from neuralplayground.comparison import GridScorer\n",
    "from neuralplayground.plotting.plot_utils import make_plot_trajectories , make_plot_rate_map, make_agent_comparison"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "data_path = fetch_model_path(\"stachenfeld_2018_in_sargolini2006\")\n",
    "\n",
    "agent_path = data_path + '/agent'\n",
    "param_path = data_path + '/params.dict'\n",
    "arena_path = data_path + '/arena'\n",
    "\n",
    "\n",
    "agent_sr = pd.read_pickle(agent_path)\n",
    "param_sr = pd.read_pickle(param_path)\n",
    "env = pd.read_pickle(arena_path)\n",
    "\n",
    "\n",
    "data_path = fetch_model_path(\"weber_2018_in_sargolini2006\")\n",
    "\n",
    "agent_path = data_path + '/agent'\n",
    "param_path = data_path + '/params.dict'\n",
    "arena_path = data_path + '/arena'\n",
    "\n",
    "\n",
    "agent = pd.read_pickle(agent_path)\n",
    "param = pd.read_pickle(param_path)\n",
    "\n",
    "\n",
    "agents=[agent_sr,agent]\n",
    "parameters=[param_sr,param]\n",
    "exp = Sargolini2006Data(verbose=False)\n",
    "env=[env]\n",
    "\n",
    "make_agent_comparison(env, parameters,agents, recording_index=0,GridScorer=GridScorer)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNU/GTK1xU7Ib59+TbtYhyH",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
