{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Tolman-Eichenbaum Machine: Unifying Space and Relational Memory through Generalization in the Hippocampal Formation.\n",
    "James C.R.Whittington, Timothy H.Muller, Shirley Mark, Guifen Chen, Caswell Barry, Neil Burgess, Timothy E.J.Behrens.\n",
    "<!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section -->\n",
    "[![All Contributors](https://img.shields.io/badge/all_contributors-4-orange.svg?style=flat-square)](#contributors-)\n",
    "<!-- ALL-CONTRIBUTORS-BADGE:END -->\n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S009286742031388X\n",
    "\n",
    "* [1 Introduction](#1-Introduction)\n",
    "* [2 Implementation](#2-Implementation)\n",
    "* [3 Running the Model](#3-RunningtheModel)\n",
    "* [4 References](#4-References)\n",
    "* [5 Contributors](#5-Contributors)\n",
    "\n",
    "For a more detailed explanation of TEM's theory and implementation in theis framework, see [Description, Implementation & Analysis of the Tolman-eichenbaum Machine](https://github.com/LukeHollingsworth/Tollman-Eichenbaum-Implementation/blob/main/Description%2C%20Implementation%20and%20Analysis%20of%20the%20Tollman-Eichenbaum%20Machine.pdf).\n",
    "\n",
    "## 1. Introduction\n",
    "The Tolman-Eichenbaum machine [1] is a model capable of generating representations that resemble some of the\n",
    "neural activity seen in the hippocampus and entorhinal cortex. The model takes inspiration from Tolman’s theory of\n",
    "an internal representation, or cognitive map [2], and combines these with the relational memory of Eichenbaum [3].\n",
    "TEM uses these concepts to both sequentially learn within an environment and to learn abstract features, common\n",
    "across different environments. Over the course of the agent’s trajectory, it learns an abstract representation of the\n",
    "spatial structure it inhabits, and as a result, is able to make predictions from state-action pairs (apple, North), even\n",
    "if it has never seen this specific pair before.\n",
    "\n",
    "The structure of TEM is biologically motivated, and reflects the interactions between key brain areas responsible\n",
    "for spatial navigation and memory. At the lowest level of the model, sensory observations *x* mirror representations\n",
    "of the lateral entorhinal cortex (LEC) [4] whilst the most abstract representations *g* are analogous to those found\n",
    "in the medial entorhinal cortex (MEC) [5]. These two representations are handled separately and are only every\n",
    "brought together to retrieve memories via the hippocampal (HPC) representation *p*.\n",
    "\n",
    "TEM has been shown to reproduce some of these well-known neural features, as well being able to remap between\n",
    "environments. The abstract EC representation *g* resembles grid cells and band cells, shown in fig. 2; by splitting *g*\n",
    "into 5 temporally-filtered streams, one is able to generate grid cells on a variety of spatial scales (modules). The\n",
    "formation of memories with place-like representations also encourages remapping across environments. Again, these\n",
    "place-like fields span multiple sizes and thus mirror the hierarchical composition of hippocampal place fields [6].\n",
    "Similarly, TEM’s HPC cells demonstrate remapping by not preserving their spatial correlation, but instead relocating\n",
    "under different environments.\n",
    "\n",
    "## 2. Implementation\n",
    "The basis of our implementation is the interaction of two classes, one for the TEM agent and the other for our\n",
    "continuous environment; these are initialised in a main file and interact within nested training loops, one for the\n",
    "training iterations, and another for the sequence of walks within an environment. The parameters of both the model\n",
    "(learning rates and sizes of HPC/EC representations etc.) and environments (width and state density etc.) can be set\n",
    "prior to training, using a separate file that is passed as an argument to both classes during initialisation. All operations\n",
    "associated with the model are carried out within the agent class, as opposed to in the original implementation where\n",
    "compression and initialisation are done outside. Both the agent and environment classes inherit from core NPG\n",
    "classes and thus can be used in the context of any other experimental setup, accounted for by our test bed. The\n",
    "ability to train on batches was a novel contribution to the NPG framework, and will be useful in the future when\n",
    "running models on given experimental trajectories.\n",
    "\n",
    "For each training iteration, the environment produces a trajectory of 25 steps, within the continuous environment;\n",
    "this trajectory is determined by the action policy, inherent to the agent class, which is used by the environment to\n",
    "determine the actions at each step. An example of these walks is given in fig. 8. The step size of the agent, as well\n",
    "as all parameters associated with the physical environment, are fully customisable. In the case of TEM, each batch\n",
    "consists of environments of varying sizes, encouraging the agent to learn the abstract structure, divorced from any\n",
    "notions of width and depth.\n",
    "\n",
    "## 3. Running the Model\n",
    "\n",
    "#### TEM Virtual Environment\n",
    "This implementation uses an older version of TensorFlow (1.9.0) and subsequently requires Python version 3.6. We suggest setting up a virtual (conda) environment with the following packages:\n",
    "```\n",
    "Python=3.6\n",
    "TensorFlow=1.9.0\n",
    "setuptools=39.0.1\n",
    "\n",
    "astropy\n",
    "matplotlib\n",
    "numpy\n",
    "seaborn\n",
    "tqdm\n",
    "sci-kit image\n",
    "scipy\n",
    "```\n",
    "\n",
    "#### Running TEM\n",
    "The TEM model is run from the [notebook file](whittington_2020_examples.ipynb). To run the model with default parameters, simply *Run All*. If you would like to alter these parameters, they can be found in the [parameters file](agents/TEM_parameters.py). The model itself can be found [here](agents/whittington_2020.py).\n",
    "\n",
    "## 4. References\n",
    "[1] J. C. Whittington, T. H. Muller, S. Mark, G. Chen, C. Barry, N. Burgess, and T. E. Behrens, “The tolman-eichenbaum machine: Unifying space and relational memory through generalization in the hippocampal forma-tion,” Cell, vol. 183, pp. 1249–1263.e23, Nov. 2020.\n",
    "\n",
    "[2] . Krupic, N. Burgess, and J. O’Keefe, “Neural representations of location composed of spatially periodic bands,\" Science, vol. 337, pp. 853–857, Aug. 2012.\n",
    "\n",
    "[3] E. C. Tolman, “Cognitive maps in rats and men.,” Psychological Review, vol. 55, no. 4, pp. 189–208, 1948.\n",
    "\n",
    "[4] S. S. Deshmukh and J. J. Knierim, “Representation of non-spatial and spatial information in the lateral entorhinal cortex,” Frontiers in Behavioral Neuroscience, vol. 5, 2011.\n",
    "\n",
    "[5] F. Savelli, D. Yoganarasimha, and J. J. Knierim, “Influence of boundary removal on the spatial representations\n",
    "of the medial entorhinal cortex,” Hippocampus, vol. 18, pp. 1270–1282, Dec. 2008.\n",
    "\n",
    "[6] M. L. Shapiro, H. Tanila, and H. Eichenbaum, “Cues that hippocampal place cells encode: Dynamic and hierarchical representation of local and distal stimuli,” Hippocampus, vol. 7, no. 6, pp. 624–642, 1997.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=“/nfs/nhome/live/lhollingsworth/Documents/Miscelaneous/TEM_image.jpg” width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sehec.arenas.TEMenv import *\n",
    "from sehec.agents.whittington_2020 import *\n",
    "from sehec.agents.TEM_extras.TEM_parameters import *\n",
    "\n",
    "pars = default_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise environment(s) and agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'TEMenv'\n",
    "mod_name = 'TEM'\n",
    "\n",
    "envs = TEMenv(environment_name=env_name, **pars)\n",
    "agent = TEM(model_name=mod_name, **pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(pars['train_iters'])):\n",
    "    adjs, trans, allowed = envs.step()\n",
    "    n_walk, _ = agent.update(adjs, trans, allowed, 0, i)\n",
    "    for j in range(n_walk - 1):\n",
    "        # RL Loop\n",
    "        adjs, trans, allowed = envs.step()\n",
    "        _, history = agent.update(adjs, trans, allowed, j+1, i)\n",
    "\n",
    "envs.plot_trajectory(history_data=history)\n",
    "plt.savefig('saved_plot_trajectory{0}.png'.format(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import datetime\n",
    "import re\n",
    "from os import listdir\n",
    "import sys\n",
    "sys.path.insert(0, '../Summaries')\n",
    "import sehec.agents.TEM_extras.TEM_plotting_functions\n",
    "from sehec.agents.TEM_extras.TEM_arb_functions import *\n",
    "from sehec.agents.TEM_extras.TEM_helper_functions import *\n",
    "from sehec.agents.TEM_extras.TEM_behaviour_analyses import *\n",
    "from sehec.agents.TEM_extras.TEM_environment_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*FIX SAVE_DIRS PROBLEM*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dirs = ['/nfs/nhome/live/lhollingsworth/Documents/NeuralPlayground/EHC_model_comparison/sehec/models/Summaries/']\n",
    "\n",
    "date = '2022-11-01'\n",
    "run = '1'\n",
    "\n",
    "recent = -1\n",
    "data, para, list_of_files, save_path = TEM_plotting_functions.get_data(save_dirs, run, date, recent)\n",
    "\n",
    "A_RNN, x_all, g_all, p_all, p_gen_all, acc_s_t_to, acc_s_t_from, positions, timeseries = data\n",
    "params, widths, batch_id, g_size, p_size, s_size, s_size_comp, n_freq, width, states = para\n",
    "print(params['world_type'])\n",
    "\n",
    "mult = 4 if params['world_type'] == 'tonegawa' else 4  # upsample\n",
    "smoothing = 1\n",
    "cmap = 'jet'\n",
    "maxmin=True\n",
    "\n",
    "seaborn.set_style(style='white')\n",
    "seaborn.set_style({'axes.spines.bottom': False,'axes.spines.left': False,'axes.spines.right': \\\n",
    "                   False,'axes.spines.top': False})\n",
    "\n",
    "masks, g_lim, p_lim = TEM_plotting_functions.sort_data(g_all, p_all, widths, mult, smoothing, params, batch_id, \\\n",
    "                                         g_max_0=False, p_max_0=True)\n",
    "\n",
    "env0 = 1\n",
    "env1 = 2\n",
    "env2 = 3\n",
    "env3 = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent coverage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "for i, env in enumerate([env2, env3]):\n",
    "    plt.subplot(1,2,i+1)\n",
    "    cell_reshaped = reshape_cells(positions[env], widths[batch_id[env]], params['world_type'])\n",
    "    plt.imshow(cell_reshaped)\n",
    "    plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(min(positions[env0]), min(positions[env1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy Map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i, env in enumerate([env0, env1]):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    cell_reshaped = reshape_cells(acc_s_t_to[env], widths[batch_id[env]], params['world_type'])\n",
    "    plt.imshow(cell_reshaped,vmax=1,vmin=0)\n",
    "    plt.title('accuracy to')\n",
    "\n",
    "    plt.colorbar()\n",
    "    plt.subplot(2,2,i+3)\n",
    "    cell_reshaped = reshape_cells(acc_s_t_from[env], widths[batch_id[env]], params['world_type'])\n",
    "    plt.imshow(cell_reshaped,vmax=1,vmin=0)\n",
    "    plt.title('accuracy from')\n",
    "\n",
    "    plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entorhinal Cortex\n",
    "Grid cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_plot(g_all[env0], widths[batch_id[env0]], name='g0', maxmin=maxmin, shiny=None, \\\n",
    "            hexy=params['world_type'], lims=g_lim, mult=mult, smoothing=smoothing, cmap=cmap, mask=masks[env0])\n",
    "\n",
    "square_plot(g_all[env1], widths[batch_id[env1]], name='g1', maxmin=maxmin, shiny=None, \\\n",
    "            hexy=params['world_type'], lims=g_lim, mult=mult, smoothing=smoothing, cmap=cmap, mask=masks[env1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autocorrelations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_autocorr_plot(g_all[env0], widths[batch_id[env0]], name='g0_auto', \\\n",
    "                     hexy=params['world_type'], mult=mult, smoothing=smoothing, cmap=cmap, circle=True)\n",
    "\n",
    "square_autocorr_plot(g_all[env1], widths[batch_id[env1]], name='g1_auto', \\\n",
    "                     hexy=params['world_type'], mult=mult, smoothing=smoothing, cmap=cmap, circle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hippocampus\n",
    "Place cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_plot(p_all[env0],widths[batch_id[env0]], name='p0', shiny=None,\\\n",
    "            hexy=params['world_type'], lims=p_lim, mult=mult, smoothing=smoothing, cmap=cmap, mask=masks[env0])\n",
    "\n",
    "square_plot(p_all[env1],widths[batch_id[env1]], name='p1', shiny=None, \\\n",
    "            hexy=params['world_type'], lims=p_lim, mult=mult, smoothing=smoothing, cmap=cmap, mask=masks[env1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioural Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.set_style(style='white')\n",
    "\n",
    "params['acc_simu'] = 1  # how accurate simulated node/edge agent is\n",
    "recent = -1  # how far back into history of saved data\n",
    "filt_size = 61  # smoothing window size (must be odd)\n",
    "n = 10\n",
    "fracs = [x /n for x in range(n+2)]  # for assessing accuracy within certain proportions of nodes visited\n",
    "\n",
    "# for steps since visted analysis - assess accuracy within those steps\n",
    "if params['world_type'] in ['family_tree', 'line_ti', 'tonegawa']:\n",
    "    a_s = [0, 10, 20]\n",
    "else:\n",
    "    a_s = [0, 4, 10, 20, 40, 60, 100, 200, 300, 400, 600]\n",
    "\n",
    "# Load data\n",
    "positions_link, coos, env_info, distance_info = link_inferences(save_path, list_of_files, widths, batch_id, params,\\\n",
    "                                                                index=recent)\n",
    "n_states, wids = env_info\n",
    "\n",
    "n_available_states = np.zeros_like(wids)\n",
    "n_available_edges = np.zeros_like(wids)\n",
    "\n",
    "n_available_edgess = [460, 460, 561, 561, 288, 369, 460, 561, 288, 369, 460, 561, 288, 288, 369, 369]\n",
    "n_available_statess = [100, 100, 121, 121, 64, 81, 100, 121, 64, 81, 100, 121, 64, 64, 81, 81]\n",
    "\n",
    "for i in range(len(n_available_edgess)):\n",
    "    n_available_edges[i] = n_available_edgess[i]\n",
    "\n",
    "for i in range(len(n_available_statess)):\n",
    "    n_available_states[i] = n_available_statess[i]\n",
    "\n",
    "\n",
    "# Perform behavioural analayses. Partition results into environments of same size\n",
    "allowed_widths = sorted(np.unique([widths[b_id] for b_id in batch_id]))\n",
    "results = []\n",
    "for allowed_wid in allowed_widths:\n",
    "    p_cors, nodes_visited_all, edges_visited_all, time_vis_anal = \\\n",
    "        analyse_link_inference(allowed_wid, fracs, a_s, positions_link, coos, env_info, params, n_available_edges, n_available_states)\n",
    "    p_cors = [ind for ind in p_cors if len(ind)>0]\n",
    "    results.append([p_cors, nodes_visited_all, edges_visited_all, time_vis_anal])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_vs_sum_nodes_edges(results, allowed_widths, coos, filt_size, wids, n_available_states, n_available_edges)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cb711b48ea87f2e3c60ae38ef0a8b23537c6a4193bdfbbf3f909425b2151f03b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
