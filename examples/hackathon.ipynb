{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# UniReps & NeuroAI Hackathon: <br> Aligning Neural Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### How do we compare computational models with the brain?\n",
    "- Understanding how the brain works often requires comparing **different models** with **experimental data**.  \n",
    "- These comparisons can be made at multiple levels:\n",
    "  - **Behavioral** patterns  \n",
    "  - **Neural activity**  \n",
    "- To make such comparisons, we need a way to relate the **abstract numbers from models** to **experimental observations**.  \n",
    "- The **goal of this hackathon** is to explore the considerations required for making these comparisons correctly, such that we can get insights into the **inner workings of the brain** through modeling  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### How do researchers compare models with the brain?\n",
    "\n",
    "| **Method** | **What It Tests** | **How It’s Done** | **Key References** |\n",
    "|------------|-------------------|-------------------|--------------------|\n",
    "| **Tuning Curve Comparisons** | Similarity of single-neuron selectivity | Fit and compare tuning functions (e.g., orientation, direction) | [Hubel & Wiesel (1962)](https://pubmed.ncbi.nlm.nih.gov/14449617/) ; [Carandini et al. (2005)](https://www.jneurosci.org/content/25/46/10577) |\n",
    "| **Encoding Models** | Predictive power of model features for neural activity | Regression from model features → neural responses | [Naselaris et al. (2011)](https://pubmed.ncbi.nlm.nih.gov/20691790/) ; [Yamins et al. (2014)](https://www.pnas.org/doi/10.1073/pnas.1403112111) ; [BrainScore (2018)](https://www.biorxiv.org/content/10.1101/407007v1) |\n",
    "| **Decoding / Behavioral Alignment** | Similar readout performance from population activity | Classify/decipher stimuli or task variables from activity | [Hung et al. (2005)](https://pubmed.ncbi.nlm.nih.gov/16272124/) ; [Cichy et al. (2016)](https://www.nature.com/articles/srep27755) |\n",
    "| **Representational Similarity Analysis (RSA)** | Geometry of stimulus representations | Compare representational dissimilarity matrices (RDMs) | [Kriegeskorte et al. (2008)](https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008/full) ; [Khaligh-Razavi & Kriegeskorte (2014)](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003915) |\n",
    "| **Dimensionality & Population Structure** | Low-dimensional manifold structure | PCA, factor analysis, demixed PCA, manifold alignment | [Churchland et al. (2012)](https://www.nature.com/articles/nature11129) ; [Stringer et al. (2019)](https://www.nature.com/articles/s41586-019-1346-5) ; [Pandarinath et al. (2018)](https://www.jneurosci.org/content/38/44/9390.abstract) |\n",
    "| **Dynamics Matching** | Temporal evolution & computational motifs | Dynamical systems analysis, fixed points, attractors | [Sussillo & Barak (2013)](https://pubmed.ncbi.nlm.nih.gov/23272922/) ; [Mante et al. (2013)](https://www.nature.com/articles/nature12742) |\n",
    "| **Information-Theoretic Comparisons** | Coding efficiency & information content | Mutual information, Fisher information, entropy | [Quiroga & Panzeri (2009)](https://www.nature.com/articles/nrn2578) ; [Borst & Theunissen (1999)](https://www.nature.com/articles/nn1199_947) |\n",
    "| **Task-Performance Equivalence** | Same behavior under same task | Compare psychometric functions, error distributions | [Yamins & DiCarlo (2016)](https://www.nature.com/articles/nn.4244) ; [Richards et al. (2019)](https://www.nature.com/articles/s41593-019-0520-2) |\n",
    "| **Causal Interventions / Perturbations** | Functional role of units | Virtual lesions, noise injection, compare to optogenetics | [Sajid et al. (2021)](https://pmc.ncbi.nlm.nih.gov/articles/PMC8018968/?utm_source=chatgpt.com) ; [Alstott et al. (2009)](https://pubmed.ncbi.nlm.nih.gov/19521503/) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### In this hackathon we will explore the Procrustes Alignment to compare the model with neural recordings\n",
    "\n",
    "* **Procrustes alignment** is a method for comparing the **geometry of population activity** between a model and a set of real neural recordings. It helps us determine if a model's representation of neural activity is similar to what's observed in the brain.\n",
    "* To illustrate its utility, we'll use Procrustes alignment to compare grid cell activity from an experiment with the neural representations from two different models.\n",
    "* The necessary tools for this demonstration are available in the **NeuralPlayground** package. However, you are encouraged to write your own implementations as you go.\n",
    "* We welcome any feedback on how we can improve the **NeuralPlayground** package!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Hackathon Content\n",
    "\n",
    "1. **Installing NeuralPlayground**: We will start by setting up the necessary environment and installing the `NeuralPlayground` package.\n",
    "2. **Accessing Experimental Data**: We'll get access to experimental data from the Sargolini et al. (2006) study, which contains recorded grid cell activity.\n",
    "3. **Loading Pre-trained Models**: You will be able to load pre-trained models (or you can train it yourself!) to get a theoretical representation of grid cell activity. One model corresponding to the Successor Representation (SR from [Stachenfeld et al. 2017](https://www.nature.com/articles/nn.4650)), and an excitatory and inhibitory plasticity model (EI from [Weber & Sprekeler 2018](https://elifesciences.org/articles/34560))\n",
    "4. **Comparing Model and Experimental Data**: The final step is to use Procrustes alignment to compare the model's neural activity with the experimental grid cell data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1. Setting up the NeuralPlayground Environment\n",
    "\n",
    "- This installation requires **Python**, **pip** and **git**.  \n",
    "- For a lightweight setup, we recommend using **Miniconda**.  \n",
    "- If you don’t have Miniconda installed, follow the instructions below.  \n",
    "- If you already have Miniconda, simply activate your environment and run:  \n",
    "\n",
    "```bash\n",
    "git clone https://github.com/SainsburyWellcomeCentre/NeuralPlayground\n",
    "cd NeuralPlayground\n",
    "pip install -e .[dev]\n",
    "```\n",
    "pip will install all the necessary requirements to run the Jupyter notebook, including: scipy, matplotlib, torch (We do have an older version that you can install with `pip install neuralplayground`, but the version in the repo is straight from the oven! with new features specifically done for this hackathon)\n",
    "\n",
    "If you already have an environment with these dependencies installed, you can use that instead.\n",
    "\n",
    "**After installing NeuralPlayground you can jump to Section 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Install Miniconda and Environment\n",
    "\n",
    "Miniconda is a minimal installer for Conda, providing a lightweight way to get started with Python and package management.\n",
    "You can find the [original instructions here](https://www.anaconda.com/docs/getting-started/miniconda/install), or you can folow the ones below.\n",
    "\n",
    "* **Download the installer:**\n",
    "    * **macOS:** Open your terminal and use `curl` to download the installer:\n",
    "        ```bash\n",
    "        curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh\n",
    "        ```\n",
    "        <br>\n",
    "    * **Linux:** Open your terminal and use `wget` (or `curl` if `wget` isn't available) to download the installer:\n",
    "        ```bash\n",
    "        wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "        ```\n",
    "        <br>\n",
    "    * **Windows:** Download the installer directly from the official Miniconda website using your web browser:\n",
    "        [https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe](https://www.google.com/search?q=https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "* **Run the installer:**\n",
    "\n",
    "    * **macOS/Linux:**\n",
    "        ```bash\n",
    "        bash Miniconda3-latest-MacOSX-x86_64.sh # For macOS\n",
    "        # OR\n",
    "        bash Miniconda3-latest-Linux-x86_64.sh # For Linux\n",
    "        ```\n",
    "        <br>\n",
    "        Follow the prompts: press `ENTER` to accept the license, type `yes` to accept the license terms, and press `ENTER` to confirm the installation location. When asked if you want to initialize Conda, type `yes`.\n",
    "        <br>\n",
    "      \n",
    "    * **Windows:** Double-click the downloaded `.exe` file and follow the on-screen instructions. Choose \"Just Me\" for installation unless you need it for all users, and accept the default installation location. Ensure you check \"Add Miniconda3 to my PATH environment variable\" during installation, though it's often recommended to avoid this and use the Anaconda Prompt instead.\n",
    "\n",
    "* **Restart your terminal/shell:**\n",
    "\n",
    "    * **macOS/Linux:** Close and reopen your terminal or run `source ~/.bashrc` (Linux) or `source ~/.zshrc` (macOS with Zsh) to apply the changes.\n",
    "\n",
    "    * **Windows:** Open a new **Anaconda Prompt (Miniconda3)** from your Start Menu. This prompt is specifically configured to use your Conda installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Create a Conda Environment\n",
    "\n",
    "Creating separate environments helps manage dependencies for different projects and avoids conflicts.\n",
    "\n",
    "* **Create a new environment:** Replace `myenv` with your desired environment name and `python=3.13` with your preferred Python version.\n",
    "    ```bash\n",
    "    conda create -n myenv python=3.13\n",
    "    ```\n",
    "    <br>\n",
    "* **Activate the environment:**\n",
    "\n",
    "    ```bash\n",
    "    conda activate myenv\n",
    "    ```\n",
    "    <br>\n",
    "    You'll see the environment name in your terminal or Anaconda Prompt (e.g., `(myenv) your_username@your_machine:~$` or `(myenv) C:\\Users\\your_username>`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Install Pip\n",
    "\n",
    "`pip` is Python's standard package installer. It allows you to install packages that might not be available through Conda channels.\n",
    "\n",
    "* **Install pip in your active environment:**\n",
    "\n",
    "    ```bash\n",
    "    conda install pip\n",
    "    ```\n",
    "    <br>\n",
    "    Alternatively, `pip` is often included by default when you create an environment with a Python version. You can verify its presence and version by running:\n",
    "\n",
    "    ```bash\n",
    "    pip --version\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "From here just clone the repo and install as an editor.\n",
    "```bash\n",
    "git clone https://github.com/SainsburyWellcomeCentre/NeuralPlayground\n",
    "cd NeuralPlayground\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 2. Accessing Experimental Data\n",
    "\n",
    "As mentioned before, in this hackathon we will use **grid cells** as examples.  \n",
    "**Grid cells** are a type of neuron found in the **medial entorhinal cortex (MEC)** of the brain, first discovered in 2005 by [Hafting et al. (2005)](https://www.nature.com/articles/nature03721). \n",
    "\n",
    "These cells are remarkable for their unique firing patterns: as an animal moves through its environment, a grid cell activates at multiple locations that form a **hexagonal grid-like pattern** across space.\n",
    "\n",
    "This spatially periodic activity makes grid cells essential for **navigation** and **spatial memory**.  \n",
    "Unlike **place cells**, which fire at specific locations ([O’Keefe & Dostrovsky, 1971](https://www.science.org/doi/10.1126/science.171.3967.208)), grid cells provide a **coordinate system** that can generalize across different environments.\n",
    "\n",
    "Grid cells have become one of the most studied neural representations due to their:\n",
    "\n",
    "- **Structured and regular firing patterns** ([Moser et al. 2014](https://www.nature.com/articles/nrn3766))\n",
    "- **Role in path integration** (estimating position based on self-motion) ([McNaughton et al., 2006](https://www.nature.com/articles/nrn1932))\n",
    "- **Relevance to models of cognitive mapping and spatial encoding** ([Moser et al., 2008](https://www.annualreviews.org/doi/10.1146/annurev.neuro.31.061307.090723))\n",
    "\n",
    "Understanding grid cells has been crucial in bridging **neuroscience and computational models**, especially in exploring how the brain constructs internal maps of the external world.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Conjunctive Representation of Position, Direction, and Velocity in Entorhinal Cortex (Sargolini et al., 2006)\n",
    "\n",
    "**Experimental Setup**\n",
    "\n",
    "- **Subjects**: Rats exploring a two-dimensional open environment.\n",
    "- **Recording Sites**: Principal neuron layers (II, III, V, VI) of medial entorhinal cortex (MEC).\n",
    "- **Method**:\n",
    "  - Multi-layer neuronal recordings using tetrodes in dorsal MEC of 17 rats.\n",
    "  - Rats moved freely in a square arena (two 10-minute sessions with a 10-minute interval between).\n",
    "  - Neural activity tracked relative to position, head direction, and running speed.\n",
    "- **Cells Identified**:\n",
    "  - **Grid cells** (periodic spatial firing) present in **all layers**, particularly dense in layer II.\n",
    "  - **Head-direction cells** (tuned to facing direction).\n",
    "  - **Conjunctive cells** that simultaneously code for **position**, **head direction**, and **velocity** in deeper layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**Main Conclusions**\n",
    "\n",
    "- **Layer-specific representations**:\n",
    "  - Layer II dominated by pure grid cells;\n",
    "  - Deeper layers feature mixtures—grid, head-direction, and conjunction neurons.\n",
    "- **Speed modulation**:\n",
    "  - All identified cell types (grid, head-direction, conjunctive) exhibit firing rate modulation by the animal’s running speed.\n",
    "- **Conjunctive coding supports self-motion updating**:\n",
    "  - The integration of spatial location, head direction, and velocity within single units provides a robust mechanism for **path-integration**, enabling the navigation system to update spatial coordinates continuously during movement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Let's look at some grid cells from Sargolini et al. 2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from neuralplayground.experiments import Sargolini2006Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "- Set `verbose=True` for more detailed information.  \n",
    "- By default, `fetch_subset=True` downloads only a subset of the dataset.  \n",
    "- If you set `fetch_subset=False`, the **entire dataset** (≈2.38 GB) will be downloaded.  \n",
    "- In this notebook, we will use only a few neurons, but if you want to explore the full dataset, set `fetch_subset=False`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sargolini_data = Sargolini2006Data(verbose=False, fetch_subset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe = sargolini_data.show_data(full_dataframe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sargolini_data = Sargolini2006Data(verbose=False)\n",
    "help(sargolini_data.plot_spike_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "# This returns positions x, y, time in seconds and spike times\n",
    "x, y, time_array, spikes = sargolini_data.plot_spike_train(recording_index=4, tetrode_id=\"T5C2\", ax=ax)\n",
    "ax.set_xlim([200, 250])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "# This returns positions x, y, time in seconds and spike times\n",
    "x, y, time_array, spikes = sargolini_data.plot_spike_train(recording_index=4, tetrode_id=\"T8C2\", ax=ax)\n",
    "ax.set_xlim([200, 250])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "# Average firing rate for different places in the 2D arena\n",
    "rate_map, x_bin, y_bin = sargolini_data.plot_recording_tetr(ax=ax, recording_index=4, tetrode_id=\"T6C1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "# Average firing rate for different places in the 2D arena\n",
    "rate_map, x_bin, y_bin = sargolini_data.plot_recording_tetr(ax=ax, recording_index=4, tetrode_id=\"T8C2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Now you have access to all neurons in the dataset!\n",
    "\n",
    "We encourage you to examine the different types of cells observed. \n",
    "- Are they all strongly grid-like? How many distinct types appear?\n",
    "- How should we classify the others? Are the patterns consistent across animals?\n",
    "\n",
    "If you want to explore additional datasets and features of the `Experiment` class,  \n",
    "please check out [our notebook with more examples](https://github.com/SainsburyWellcomeCentre/NeuralPlayground/blob/main/examples/experimental_examples/experimental_data_examples.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Creating an environment to train each model\n",
    "\n",
    "- Before training the models used in this hackathon, we will first create an environment.  \n",
    "\n",
    "- This package allows you to:  \n",
    "  - Import **pre-made environments** with the correct dimensions, similar to the experiment.  \n",
    "  - Automatically load the **behavioral data**, giving access to the animal’s behavior, which can later be used to train the agent. For that you can set `use_behavioral_data=True`  \n",
    "  - Use the **default policy** of each agent, which imitates naturalistic behavior, `use_behavioral_data=False`\n",
    "\n",
    "This environment is quite simple and serves the purpose of the hackathon, but you could try training agents in more complicated environments, or even make your own! Please check our [jupyter notebook](https://github.com/SainsburyWellcomeCentre/NeuralPlayground/blob/main/examples/arena_examples/arena_examples.ipynb) with examples on how to built custom environments.\n",
    "\n",
    "You can experiment with different types of arenas. They are typically designed to resemble those used when recording grid cells, but feel free to try variations and compare the outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from neuralplayground.arenas import Sargolini2006\n",
    "\n",
    "env_name = \"Sargolini2006\"\n",
    "time_step_size = 0.1 #seg\n",
    "agent_step_size = 5\n",
    "\n",
    "env = Sargolini2006(environment_name = env_name,\n",
    "                    time_step_size = time_step_size,\n",
    "                    agent_step_size = agent_step_size,\n",
    "                    use_behavioral_data = True)\n",
    "\n",
    "env.plot_recorded_trajectory(recording_index=3, plot_every=5)\n",
    "\n",
    "# We can set this trajectory to be used by the agent by setting \n",
    "env.set_animal_data(recording_index=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 3. Training agents\n",
    "\n",
    "- We now move on to training the agents.  \n",
    "\n",
    "- Our goal is to compare the **grid-cell representations** of two models against experimental data:  \n",
    "  - **Successor Representation (SR) model** from [Stachenfeld et al., 2017](https://www.nature.com/articles/nn.4650).  \n",
    "  - **Excitatory/Inhibitory Plasticity model** from [Weber & Sprekeler, 2018](https://elifesciences.org/articles/34560).  \n",
    "\n",
    "- Both models provide theoretical mechanisms for the emergence of grid cells:  \n",
    "  - The SR model explains grid cells as eigenvectors of the successor representation matrix.  \n",
    "  - The plasticity model explains them as the result of the balance between excitatory and inhibitory plasticity.  \n",
    "\n",
    "- We will train these models using **animal trajectory data** and then compare the **emergent grid-cell representations** to neural recordings.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Summary of the Successor Representation (SR), [Stachenfeld et al. 2017](https://www.nature.com/articles/nn.4650)\n",
    "\n",
    "- **Successor Representation (SR):**  \n",
    "  Consider an environment modeled as a Markov chain with transition matrix $T$ under some policy $\\pi$.  \n",
    "  The SR is defined as the expected discounted future occupancy of states:  \n",
    "\n",
    "  $$\n",
    "  M(s, s') = \\mathbb{E}\\left[\\sum_{t=0}^\\infty \\gamma^t \\, \\mathbb{I}\\{s_t = s'\\} \\mid s_0 = s\\right]\n",
    "  $$\n",
    "\n",
    "- **Derivation of Matrix Form:**  \n",
    "  Expanding the expectation over transitions:  \n",
    "\n",
    "  $$\n",
    "  M = I + \\gamma T + \\gamma^2 T^2 + \\gamma^3 T^3 + \\cdots\n",
    "  $$\n",
    "\n",
    "  This is a geometric series in matrices, which converges when $\\gamma < 1$:  \n",
    "\n",
    "  $$\n",
    "  M = (I - \\gamma T)^{-1}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "- **Decomposing Value Function:**  \n",
    "  Given a reward vector $R$, the value of state $s$ is:  \n",
    "\n",
    "  $$\n",
    "  V(s) = \\sum_{s'} M(s, s') \\, R(s')\n",
    "  $$\n",
    "\n",
    "  or in vector form:  \n",
    "\n",
    "  $$\n",
    "  V = M R\n",
    "  $$\n",
    "\n",
    "- **Eigen-decomposition and Grid Cells:**  \n",
    "  The SR can be decomposed as:  \n",
    "\n",
    "  $$\n",
    "  M = \\Phi \\Lambda \\Phi^{-1}\n",
    "  $$\n",
    "\n",
    "  where the eigenvectors in $\\Phi$ form spatial basis functions. Many of these exhibit periodic, wave-like structure resembling grid cell firing fields.\n",
    "\n",
    "- **Interpretation of Eigenvectors:**  \n",
    "  - **Low-frequency eigenvectors**: capture broad spatial structure (hierarchical planning, subgoals).  \n",
    "  - **High-frequency eigenvectors**: produce fine, periodic patterns akin to entorhinal grid cells.\n",
    "    <br>\n",
    "    \n",
    "- **Functional Significance:**  \n",
    "  Grid cells are thus modeled as the **eigenvectors of the SR**, providing a low-dimensional basis for efficient representation, planning, and noise robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from neuralplayground.agents import Stachenfeld2018\n",
    "\n",
    "agent_step_size = 5\n",
    "discount = .9\n",
    "threshold = 1e-6\n",
    "lr_td = 1e-1\n",
    "t_episode = 1000\n",
    "n_episode = 100\n",
    "state_density = (1 / agent_step_size)\n",
    "twoDvalue = True\n",
    "\n",
    "SR_agent = Stachenfeld2018(discount=discount, t_episode=t_episode, n_episode=n_episode, threshold=threshold, lr_td=lr_td,\n",
    "                           room_width=env.room_width, room_depth=env.room_depth, state_density=state_density, twoD=twoDvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read the implementation of each of this agent in\n",
    "https://github.com/SainsburyWellcomeCentre/NeuralPlayground/blob/main/neuralplayground/agents/stachenfeld_2018.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One can compute the successor representation using successive additive update\n",
    "print(\"Additive updates method\")\n",
    "sr = SR_agent.update_successor_rep_td_full(n_episode=100, t_episode=100) \n",
    "SR_agent.plot_rate_map(SR_agent.srmat_full_td, eigen_vectors=[1,10,15,20,25])\n",
    "SR_agent.plot_rate_map(sr, eigen_vectors=[1,10,15,20,25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One can compute the successor representation matrix using geometric sums for $\\gamma<1$\n",
    "print(\"Geometric Sum method\")\n",
    "sr_sum = SR_agent.successor_rep_sum() \n",
    "SR_agent.plot_rate_map(SR_agent.srmat_sum, eigen_vectors=[1,10,15,20,25])\n",
    "SR_agent.plot_rate_map(sr_sum, eigen_vectors=[1,10,15,20,25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Closed form solution of the successor representation\")\n",
    "srmat_ground = SR_agent.successor_rep_solution()\n",
    "SR_agent.plot_rate_map(SR_agent.srmat_ground, eigen_vectors=[1,10,15,20,25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Note that in all of the above, the method used to compute $M$ does not rely on the environment. Instead, it assumes a 2D space where all transitions between nearby places are possible. This assumption may not always hold, and in such cases, we need to train the agent in an environment as follows.\n",
    "\n",
    "You could also train this agent in other environments, as in https://github.com/SainsburyWellcomeCentre/NeuralPlayground/blob/main/examples/arena_examples/arena_examples.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_every = 200000\n",
    "total_iters = 0\n",
    "obs, state = env.reset()\n",
    "for i in tqdm(range(1000000)):\n",
    "# Observe to choose an action\n",
    "    action = SR_agent.act(obs[:2])  # the action is link to density of state to make sure we always land in a new\n",
    "    SR_agent.update()\n",
    "    obs, state, reward = env.step(action)\n",
    "    obs= obs[:2]\n",
    "    total_iters += 1\n",
    "    if total_iters % plot_every == 0:\n",
    "        SR_agent.plot_rate_map(sr_matrix=SR_agent.srmat, eigen_vectors=[1,10,15,20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Summary of place cells and grid cells with excitatory and inhibitory plasticity\n",
    "\n",
    "[Weber & Sprekeler 2018](https://elifesciences.org/articles/34560)\n",
    "\n",
    "In this work, the authors propose a theoretical model in which **place cells and grid cells** emerge from the interaction of excitatory and inhibitory plasticity. The spatial structure of the output cells depends critically on the relative smoothness of the excitatory and inhibitory tuning functions, $r_i^{E}(x)$ and $r_j^{I}(x)$, where $x$ denotes the animal’s position.\n",
    "\n",
    "\n",
    "## Output activity\n",
    "\n",
    "The activity of the output neuron is given by\n",
    "\n",
    "$$\n",
    "r^{out}(x(t)) = \\Bigg[ \\sum_{i=1}^{N_{e}} w_i^{E}(t) \\, r_i^{E}(x(t)) \\;-\\; \\sum_{j=1}^{N_{i}} w_j^{I}(t) \\, r_j^{I}(x(t)) \\Bigg]_{+},\n",
    "$$\n",
    "\n",
    "where $N_e$ and $N_i$ are the numbers of excitatory and inhibitory inputs, $w_i^E$ and $w_j^I$ are their synaptic weights, and $[\\,\\cdot\\,]_{+}$ denotes rectification (ReLU nonlinearity).  \n",
    "\n",
    "For example, a typical input tuning curve can be modeled as a Gaussian:\n",
    "\n",
    "$$\n",
    "r_i^P(x) = \\alpha_P \\exp\\!\\left(-\\frac{(x - \\mu_i)^2}{2\\sigma_P^2}\\right),\n",
    "$$\n",
    "\n",
    "with random centers $\\mu_i$ and width $\\sigma_P$, where $P \\in \\{E, I\\}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Synaptic plasticity rules\n",
    "\n",
    "Weights evolve according to local Hebbian-like rules:\n",
    "\n",
    "- **Excitatory synapses:**\n",
    "\n",
    "$$\n",
    "\\Delta w^{E} = \\eta_E \\, r^{E}(x) \\, r^{out}(x),\n",
    "$$\n",
    "\n",
    "which strengthens excitatory weights whenever presynaptic input $r^E(x)$ and output activity $r^{out}(x)$ are co-active.\n",
    "\n",
    "- **Inhibitory synapses:**\n",
    "\n",
    "$$\n",
    "\\Delta w^{I} = \\eta_I \\, r^{I}(x) \\, \\big(r^{out}(x) - \\rho_0\\big),\n",
    "$$\n",
    "\n",
    "which adjusts inhibition depending on the deviation of the output from a homeostatic target firing rate $\\rho_0$.\n",
    "\n",
    "## Mechanism for grid-cell emergence\n",
    "\n",
    "The **relative smoothness** of excitatory vs. inhibitory tuning functions determines the emergent spatial patterns:\n",
    "\n",
    "- All excitatory and inhibitory rate cell ratemaps are simple Gaussians\n",
    "- If inhibitory inputs are **broader** (smoother) than excitatory ones, the output neuron develops **periodic grid-like firing fields**.  \n",
    "- If inhibitory inputs are **narrower**, localized **place-cell–like firing** emerges.  \n",
    "- Balanced smoothness can also generate band-like cells and other spatial invariances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Implementation\n",
    "\n",
    "In simulations, the position $x(t)$ can be sampled either from a random walk in 1D/2D space or from recorded animal trajectories. Using the above equations, the network self-organizes to produce **place cells, grid cells, or band cells**, depending only on the statistics of the excitatory and inhibitory inputs and their relative smoothness.\n",
    "\n",
    "See the implementation in https://github.com/SainsburyWellcomeCentre/NeuralPlayground/blob/main/neuralplayground/agents/weber_2018.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from neuralplayground.agents import Weber2018\n",
    "\n",
    "# Parameters to reproduce Figure 2a in the paper\n",
    "# \n",
    "exc_eta = 2e-4\n",
    "inh_eta = 8e-4\n",
    "model_name = \"model_example\"\n",
    "sigma_exc = np.array([0.05, 0.05])\n",
    "sigma_inh = np.array([0.1, 0.1])\n",
    "Ne = 4900\n",
    "Ni = 1225\n",
    "Nef = 1\n",
    "Nif = 1\n",
    "alpha_i = 1\n",
    "alpha_e = 1\n",
    "we_init = 1.0\n",
    "wi_init = 1.5\n",
    "agent_step_size = 0.1\n",
    "roh = 1\n",
    "W_agent = Weber2018(model_name=model_name, exc_eta=exc_eta, inh_eta=inh_eta, sigma_exc=sigma_exc,\n",
    "                    sigma_inh=sigma_inh, Ne=Ne, Ni=Ni, agent_step_size=agent_step_size, ro=roh,\n",
    "                    Nef=Nef, Nif=Nif, room_width=env.room_width, room_depth=env.room_depth,\n",
    "                    alpha_i=alpha_i, alpha_e=alpha_e, we_init=we_init, wi_init=wi_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rate maps of excitatory, inhibitory and output neuron r_out\n",
    "W_agent.plot_all_rates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" You might want to go and grab a coffee while this runs :)\n",
    "    You can also used the pre-computed responses\n",
    "\"\"\"\n",
    "plot_every = 100000\n",
    "total_iters = 0\n",
    "n_steps = 300000\n",
    "obs, state = env.reset()\n",
    "for i in tqdm(range(n_steps)):\n",
    "    # Observe to choose an action, the first to numbers in the observation \n",
    "    # are the xy position of the agent\n",
    "    obs = obs[:2]\n",
    "    action = W_agent.act(obs)\n",
    "    W_agent.update()\n",
    "    # Run environment for given action\n",
    "    obs, state, reward = env.step(action, normalize_step=True)\n",
    "    total_iters += 1\n",
    "    if i % plot_every == 0:\n",
    "        W_agent.plot_all_rates()\n",
    "\n",
    "W_agent.plot_all_rates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Play with the Models!\n",
    "\n",
    "Take some time to experiment with the models by adjusting their parameters.  \n",
    "For example, try changing the size of the environment or other settings, and observe how these changes affect the results.  \n",
    "\n",
    "- What differences do you notice between the two agents?  \n",
    "- Which one do you think is *better*?  \n",
    "\n",
    "Finally, discuss the advantages and disadvantages of each model.  \n",
    "Consider aspects such as efficiency, flexibility, and how well they capture features of real neural data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Since the plasticity model takes some time to run, we provide additional grid cell rate maps generated with different seeds at four training stages (0, 1e5, 2e5, and 3e5 iterations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weber_grid_cells = np.load(\"weber_grids.npy\")\n",
    "# weber_grid_cells.shape = (n_seeds, bins_x, bins_y, train_stage)\n",
    "\n",
    "# Let's plot some of them\n",
    "plot_n_cells = 3\n",
    "f, ax = plt.subplots(plot_n_cells, 4)\n",
    "for i in range(plot_n_cells):\n",
    "    ax[i, 0].imshow(weber_grid_cells[i, :, :, 0], cmap=\"jet\")\n",
    "    ax[i, 1].imshow(weber_grid_cells[i, :, :, 1], cmap=\"jet\")\n",
    "    ax[i, 2].imshow(weber_grid_cells[i, :, :, 2], cmap=\"jet\")\n",
    "    ax[i, 3].imshow(weber_grid_cells[i, :, :, 3], cmap=\"jet\")\n",
    "    ax[i, 0].set_ylabel(\"Seed \"+str(i))\n",
    "\n",
    "ax[0, 0].set_title(\"0 iters\")\n",
    "ax[0, 1].set_title(\"1e5 iters\")\n",
    "ax[0, 2].set_title(\"2e5 iters\")\n",
    "ax[0, 3].set_title(\"3e5 iters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weber_grid_cells.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Neural Representation Comparison\n",
    "Now that we have neural representations from models, and recorded grid cells for similar environments, can we compare the representations? In this case, we will compare the firing rate at each point in space from the model and from the data. To do this, we adopt the Procrustes Alignment which we explain now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## What is Procrustes Alignment\n",
    "Procrustes alignment is a method from shape analysis. Given two sets of points in a high-dimensional space, e.g., neural recordings to different stimuli (points in space in our case) vs. model units activity, it finds the **best affine transformation** (rotation, reflection, scaling, translation) that minimizes the distance between corresponding points.\n",
    "\n",
    "Formally, given matrices $X$ (neural recordings, here real grid cells) and $Y$ (model responses, here simulated grid cells), Procrustes finds $(R, s, t)$ such that:\n",
    "\n",
    "$$\\min_{R, s, t} \\; \\| X - (s YR + t) \\|$$\n",
    "\n",
    "where:\n",
    "- $R$ is a rotation/reflection,\n",
    "- $s$ is a scaling factor,\n",
    "- $t$ is a translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Why is the Procrustes Alignment a Good Metric for Comparing Neural Representations?\n",
    "\n",
    "Geometrical figures are invariant to translation, scaling, and rotation. This means that, for instance, if you apply any of these transformations to a shape, the shape itself remains the same. Think, for example, of a triangle or any other geometrical shape: moving it, rotating it, or resizing it (scaling) preserves the features that define its shape.  \n",
    "\n",
    "Therefore, given the geometry of neural recordings (represented by the points in $X$), if you can find a transformation, rotation, scaling, and translation, which can be represented as an affine transformation, to map the model representation $Y$ to $X$, then you can say that both representations share the same **geometrical shape**. See [Goodall 2018](https://academic.oup.com/jrsssb/article/53/2/285/7028139) for technical details of this method, and see a neuroscience application in [Williams et al. 2024](https://pmc.ncbi.nlm.nih.gov/articles/PMC10760997/) and [Ostrow et al. 2023](https://arxiv.org/pdf/2306.10168).\n",
    "\n",
    "- what does it really mean to have found a transformation?\n",
    "- How can we determine whether one transformation is better than another?\n",
    "- Does the smallest distance necessarily imply a closer match, meaning that the distributions are more similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Constructing $X$ (Neural Data) and $Y$ (Model Neurons)\n",
    "\n",
    "First, you'll need to construct your neural data matrix, $X$, and your model neuron matrix, $Y$.\n",
    "\n",
    "To create $X$, use the `sargoloni_data.tetrode_ratemap` method. This method is similar to `plot_recording_tetr` but doesn't plot the maps. Use it to obtain a few ratemaps for `N_ratemaps` grid cells from the dataset. Flatten these ratemaps and stack them along the second dimension to get the matrix `X_mec` with a shape of `(x_bins * y_bins, N_ratemaps)`.\n",
    "\n",
    "Next, do the same for the model neurons (e.g., for the Successor Representation (SR) model) to create the matrix `Y_sr`. This matrix should have a shape of `(N_states, N_ratemaps)`.\n",
    "\n",
    "Ideally, `X_mec.shape` should be equal to `Y_sr.shape`. However, the default ratemaps often have more spatial locations than the SR model. To make their shapes compatible, you'll need to adjust one of the parameters. A good way to do this is to either modify the `bin_size` argument in `sargoloni_data.tetrode_ratemap` or change the `state_density` of the SR model, or simply a 2D interpolation to get them to the same shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<span style=\"color: blue;font-size: 1.5em;\"> **Exercise** </span>\n",
    "\n",
    "Write code that generates an `X_mec` variable, where the ratemap is flattened along the first dimension, resulting in a shape of `(x_bins * y_bins, N_ratemaps)`.  \n",
    "To do this, iterate over recording indexes and tetrode IDs using the `sargoloni_data.tetrode_ratemap` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Write your code here that generates X_mec #\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<span style=\"color: blue;font-size: 1.5em;\"> **Exercise** </span>\n",
    "\n",
    "Write code that generates an `Y_sr` variable, where the ratemap is flattened along the first dimension, resulting in a shape of `(x_bins * y_bins, N_ratemaps)`.  \n",
    "To do this, iterate over different eigenvectors in the SR model to get different grid cell patterns. \n",
    "\n",
    "Note that both arrays, `Y_sr` and `X_mec` might have different shapes, find a way to make them the same size (hint: adjusting the bin size when generating `X_mec`, or interpolating `Y_sr` to have hihigher dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# Write your code here that generates Y_sr #\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## The comparison goes as follows\n",
    "\n",
    "<span style=\"color: blue;font-size: 1.5em;\"> **Exercise** </span>\n",
    "\n",
    "### Step 1: Preprocess the data\n",
    "- Standardize both datasets (`X_mec` and `X_sr`) by centering (zero mean) and scaling (unit variance).\n",
    "\n",
    "### Step 2: Procrustes Alignment\n",
    "- Apply `orthogonal_procrustes` method from scipy (which implements the solution by [Peter H. Schönemann 1996](https://link.springer.com/article/10.1007/BF02289451)) to find the best rotation matrix `R` that aligns `X_sr` to `X_mec`.\n",
    "- Because we have standardize the grid cell ratemaps, we only need to compute $$\\min_{R} \\; \\| X - YR \\|$$ subject to $R$ being a rotation matrix, $RR^T=I$.\n",
    "- Multiply `X_sr_pre` by `R` to obtain the aligned version (`X_sr_aligned`).\n",
    "\n",
    "### Step 3: Quantitative Metrics\n",
    "- Define a function to compute **average cosine similarity** between corresponding rows of two datasets.\n",
    "- Compute:\n",
    "  - Frobenius norm distance between `X_mec_pre` and both `X_sr_pre` (unaligned) and `X_sr_aligned`.\n",
    "  - Average cosine similarity before and after alignment.\n",
    "- Print these metrics for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "\n",
    "# --- At this point you should have ---\n",
    "# X_mec and X_sr must be (N_locations, N_features)\n",
    "# X_mec: real MEC data; X_sr: model representations (e.g., SR eigenvectors)\n",
    "\n",
    "# --- Step 1: Preprocess (center & scale) ---\n",
    "# Create X_mec_pre: standardize version of X_mec\n",
    "\n",
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "\n",
    "# Create Y_sr_pre: standardize version of Y_sr\n",
    "\n",
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "\n",
    "# --- Step 2: Procrustes Alignment ---\n",
    "# Compute the rotation R, and find the aligned SR representation Y_sr_aligned\n",
    "\n",
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "\n",
    "# --- Step 3: Quantitative Metrics ---\n",
    "def avg_cosine_sim(X1, X2):\n",
    "    sims = [np.dot(X1[i], X2[i]) /\n",
    "            (np.linalg.norm(X1[i]) * np.linalg.norm(X2[i]))\n",
    "            for i in range(len(X1))]\n",
    "    return np.nanmean(sims)\n",
    "\n",
    "# This will compute the similarities for you, for the aligned and unaligned Y_sr\n",
    "d_aligned = np.linalg.norm(X_mec_pre - Y_sr_aligned, 'fro')\n",
    "d_unaligned = np.linalg.norm(X_mec_pre - Y_sr_pre, 'fro')\n",
    "cos_sim_aligned = avg_cosine_sim(X_mec_pre, Y_sr_aligned)\n",
    "cos_sim_unaligned = avg_cosine_sim(X_mec_pre, Y_sr_pre)\n",
    "\n",
    "print(f\"Procrustes Distance (Aligned):   {d_aligned:.4f}\")\n",
    "print(f\"Procrustes Distance (Unaligned): {d_unaligned:.4f}\")\n",
    "print(f\"Avg Cosine Similarity (Aligned):   {cos_sim_aligned:.4f}\")\n",
    "print(f\"Avg Cosine Similarity (Unaligned): {cos_sim_unaligned:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Step 4: Random Baseline (Shuffle Test)\n",
    "- Perform a shuffle test as a null distribution:\n",
    "  - Shuffle rows of `X_sr_pre` 100 times.\n",
    "  - For each shuffle, compute Procrustes alignment, distance, and cosine similarity.\n",
    "- Collect results across shuffles.\n",
    "- Plot histograms of the null distributions and mark the actual observed values for comparison.\n",
    "\n",
    "### Step 5: Rotated ratemap inspection\n",
    "<span style=\"color: blue;font-size: 1.5em;\"> **Exercise** </span>\n",
    "- Use the transformation `R` found from the Procrustes Alignment to transform the set of grid cells from the model to have better alignment with experimental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Step 4: Random Baseline (Shuffle Test) ---\n",
    "n_shuffles = 100\n",
    "shuffle_distances = []\n",
    "shuffle_cosines = []\n",
    "\n",
    "for i in range(n_shuffles):\n",
    "    X_sr_shuffled = np.random.permutation(X_sr_pre)\n",
    "    R_rand, _ = orthogonal_procrustes(X_sr_shuffled, X_mec_pre)\n",
    "    X_sr_shuf_aligned = X_sr_shuffled @ R_rand\n",
    "\n",
    "    d_rand = np.linalg.norm(X_mec_pre - X_sr_shuf_aligned, 'fro')\n",
    "    cos_rand = avg_cosine_sim(X_mec_pre, X_sr_shuf_aligned)\n",
    "\n",
    "    shuffle_distances.append(d_rand)\n",
    "    shuffle_cosines.append(cos_rand)\n",
    "\n",
    "# Plot the null distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].hist(shuffle_distances, bins=20, color='gray', alpha=0.7)\n",
    "axes[0].axvline(d_aligned, color='red', linestyle='--', label='Actual')\n",
    "axes[0].set_title('Procrustes Distance (Null vs. Observed)')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(shuffle_cosines, bins=20, color='gray', alpha=0.7)\n",
    "axes[1].axvline(cos_sim_aligned, color='red', linestyle='--', label='Actual')\n",
    "axes[1].set_title('Cosine Similarity (Null vs. Observed)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Step 5: Rotated ratemap inspection ---\n",
    "# Show the aligned rate maps from SR, compare it with the original ones and the neural recordings\n",
    "########################\n",
    "# Write your code here #\n",
    "########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Can you compare both models (SR vs. Plasticity Network)?\n",
    "\n",
    "<span style=\"color: blue;font-size: 1.5em;\"> **Exercise** </span>\n",
    "- Apply the same method used to align the Sr to the plasticity model using the grid cells in `weber_grid_cells`.  \n",
    "- Keep in mind that the shapes are different. How would you compare them? Could you interpolate or use other methods?  \n",
    "- What can you conclude about the alignments obtained for the SR model versus the plasticity network?  \n",
    "\n",
    "Below, we plot some of these cells again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# weber_grid_cells.shape = (n_seeds, bins_x, bins_y, train_stage)\n",
    "print(weber_grid_cells.shape)\n",
    "\n",
    "# Let's plot some of them\n",
    "plot_n_cells = 3\n",
    "f, ax = plt.subplots(plot_n_cells, 4)\n",
    "for i in range(plot_n_cells):\n",
    "    ax[i, 0].imshow(weber_grid_cells[i, :, :, 0], cmap=\"jet\")\n",
    "    ax[i, 1].imshow(weber_grid_cells[i, :, :, 1], cmap=\"jet\")\n",
    "    ax[i, 2].imshow(weber_grid_cells[i, :, :, 2], cmap=\"jet\")\n",
    "    ax[i, 3].imshow(weber_grid_cells[i, :, :, 3], cmap=\"jet\")\n",
    "    ax[i, 0].set_ylabel(\"Seed \"+str(i))\n",
    "\n",
    "ax[0, 0].set_title(\"0 iters\")\n",
    "ax[0, 1].set_title(\"1e5 iters\")\n",
    "ax[0, 2].set_title(\"2e5 iters\")\n",
    "ax[0, 3].set_title(\"3e5 iters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# Write your code here to compare SR and Weber #\n",
    "################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Discussion\n",
    "\n",
    "We do not have the answers to these questions. Instead, we invite you to discuss them with others and perhaps test some of these ideas using the code provided.\n",
    "\n",
    "### Relevant considerations when comparing neural recordings and theoretical models\n",
    "- What do you think are the most important aspects researchers should consider?\n",
    "- The Procrustes alignment tests for similar geometry. What is the effect of modeling non-hexagonal grid cells? What about grid cell modularity and orientation?\n",
    "- How important is the non-negativity constraint in neural recordings? Is it relevant for modeling? [Dorrell et al. 2022](https://arxiv.org/abs/2209.15563)\n",
    "- Should a model also be able to account for noise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Data acquisition and experimental considerations\n",
    "- What is the best way to handle recordings from different animals?\n",
    "- What about recordings from different sessions? How should we account for changes over time, such as recording stability or learning?\n",
    "\n",
    "### Does it make sense to do this?\n",
    "- Suppose our model does a reasonably good job of generating grid cells for a given experiment. How can we be sure that it captures the true computational mechanisms underlying the neural recordings?\n",
    "- Could a completely different model generate similar representations? How can we tell them apart?\n",
    "- Studying neural representations in isolation is valuable. However, in this case, we are looking at a very specific subset of the entorhinal cortex. How do we integrate other (potentially) relevant brain regions or behavioral factors? Is it better to be parsimonious and study small parts in isolation, or to build more abstract models that integrate multiple regions?\n",
    "- Even if we identify the “right” model, are we not simply replacing one black box (the brain) with another (the model)? To what extent do we actually understand the models we use to describe neural recordings?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
