{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import neuralplayground.agents.george_2021_extras.george_2021_parameters as parameters\n",
    "\n",
    "# Imports\n",
    "from neuralplayground.agents.george_2021 import George2021\n",
    "from neuralplayground.arenas import DiscreteObjectEnvironment\n",
    "from neuralplayground.backend import SingleSim, default_training_loop\n",
    "from neuralplayground.experiments import Sargolini2006Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     48\u001b[39m sim = SingleSim(\n\u001b[32m     49\u001b[39m     simulation_id=simulation_id,\n\u001b[32m     50\u001b[39m     agent_class=agent_class,\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m     training_loop_params=training_loop_params\n\u001b[32m     56\u001b[39m )\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning sim...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43msim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_sim\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSim finished.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ceph/saxe/devanshi/george_2021/NeuralPlayground/neuralplayground/backend/simulation_manager.py:319\u001b[39m, in \u001b[36mSingleSim.run_sim\u001b[39m\u001b[34m(self, save_path)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m---> Training loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m trained_agent, trained_env, training_hist = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_loop_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# Saving models\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m---> Saving models\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ceph/saxe/devanshi/george_2021/NeuralPlayground/neuralplayground/backend/training_loops.py:35\u001b[39m, in \u001b[36mdefault_training_loop\u001b[39m\u001b[34m(agent, env, n_steps)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Run environment for given action\u001b[39;00m\n\u001b[32m     34\u001b[39m obs, state, reward = env.step(action)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m update_output = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m training_hist.append(update_output)\n\u001b[32m     37\u001b[39m obs = obs[:\u001b[32m2\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ceph/saxe/devanshi/george_2021/NeuralPlayground/neuralplayground/agents/george_2021.py:163\u001b[39m, in \u001b[36mGeorge2021.update\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m learning_algo == \u001b[33m\"\u001b[39m\u001b[33mEM\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Expectation-Maximization\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# term_early allows stopping if log-likelihood converges\u001b[39;00m\n\u001b[32m    162\u001b[39m     term_early = \u001b[38;5;28mself\u001b[39m.params.get(\u001b[33m\"\u001b[39m\u001b[33mterm_early\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m     convergence = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchmm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn_em_T\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43ma_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mterm_early\u001b[49m\u001b[43m=\u001b[49m\u001b[43mterm_early\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m learning_algo == \u001b[33m\"\u001b[39m\u001b[33mViterbi\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    170\u001b[39m     \u001b[38;5;66;03m# Viterbi Training (Hard EM)\u001b[39;00m\n\u001b[32m    171\u001b[39m     convergence = \u001b[38;5;28mself\u001b[39m.chmm.learn_viterbi_T(\n\u001b[32m    172\u001b[39m         x_train, \n\u001b[32m    173\u001b[39m         a_train, \n\u001b[32m    174\u001b[39m         n_iter=n_iter\n\u001b[32m    175\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ceph/saxe/devanshi/george_2021/NeuralPlayground/neuralplayground/agents/george_2021_extras/george_2021_model.py:215\u001b[39m, in \u001b[36mCHMM.learn_em_T\u001b[39m\u001b[34m(self, x, a, n_iter, term_early)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[32m    206\u001b[39m     \u001b[38;5;66;03m# E\u001b[39;00m\n\u001b[32m    207\u001b[39m     log2_lik, mess_fwd = forward(\n\u001b[32m    208\u001b[39m         \u001b[38;5;28mself\u001b[39m.T.transpose(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m),\n\u001b[32m    209\u001b[39m         \u001b[38;5;28mself\u001b[39m.Pi_x,\n\u001b[32m   (...)\u001b[39m\u001b[32m    213\u001b[39m         store_messages=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    214\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m     mess_bwd = \u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_clones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m     updateC(\u001b[38;5;28mself\u001b[39m.C, \u001b[38;5;28mself\u001b[39m.T, \u001b[38;5;28mself\u001b[39m.n_clones, mess_fwd, mess_bwd, x, a)\n\u001b[32m    217\u001b[39m     \u001b[38;5;66;03m# M\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ceph/saxe/devanshi/george_2021/NeuralPlayground/neuralplayground/agents/george_2021_extras/george_2021_model.py:524\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    520\u001b[39m message = np.ascontiguousarray(T[aij, i_start:i_stop, j_start:j_stop]).dot(\n\u001b[32m    521\u001b[39m     message\n\u001b[32m    522\u001b[39m )\n\u001b[32m    523\u001b[39m p_obs = message.sum()\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m p_obs > \u001b[32m0\u001b[39m\n\u001b[32m    525\u001b[39m message /= p_obs\n\u001b[32m    526\u001b[39m t_start, t_stop = mess_loc[t : t + \u001b[32m2\u001b[39m]\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "simulation_id = \"george_2021_test\"\n",
    "save_path = os.path.join(os.getcwd(), \"results_sim\")\n",
    "\n",
    "agent_class = George2021\n",
    "env_class = DiscreteObjectEnvironment\n",
    "training_loop = default_training_loop\n",
    "\n",
    "params = parameters.parameters()\n",
    "batch_size = 100 \n",
    "params[\"batch_size\"] = batch_size\n",
    "params[\"n_clones_per_obs\"] = 5  \n",
    "params[\"n_actions\"] = 4      \n",
    "params[\"n_iterations\"] = 5\n",
    "params[\"dtype\"] = np.float64\n",
    "\n",
    "arena_x_limits = [-5, 5]\n",
    "arena_y_limits = [-5, 5]\n",
    "state_density = 1.0\n",
    "\n",
    "room_widths = abs(arena_x_limits[1] - arena_x_limits[0])\n",
    "room_depths = abs(arena_y_limits[1] - arena_y_limits[0])\n",
    "\n",
    "n_states = int((room_widths * state_density) * (room_depths * state_density))\n",
    "\n",
    "agent_params = {\n",
    "    \"agent_name\": \"CSCG_Agent\",\n",
    "    \"params\": params,\n",
    "    \"n_observations\": n_states,\n",
    "    \"batch_size\": batch_size\n",
    "}\n",
    "\n",
    "env_params = { \n",
    "    \"environment_name\": \"DiscreteObject\",\n",
    "    \"arena_x_limits\": arena_x_limits,\n",
    "    \"arena_y_limits\": arena_y_limits,\n",
    "    \"state_density\": state_density,\n",
    "    \"n_objects\": n_states, # Map 1-to-1 for testing\n",
    "    \"agent_step_size\": 1.0,\n",
    "    \"use_behavioural_data\": False,\n",
    "    \"data_path\": None,\n",
    "    \"experiment_class\": Sargolini2006Data\n",
    "}\n",
    "\n",
    "training_loop_params = {\n",
    "    \"n_steps\": 2000\n",
    "}\n",
    "\n",
    "sim = SingleSim(\n",
    "    simulation_id=simulation_id,\n",
    "    agent_class=agent_class,\n",
    "    agent_params=agent_params,\n",
    "    env_class=env_class,\n",
    "    env_params=env_params,\n",
    "    training_loop=training_loop,\n",
    "    training_loop_params=training_loop_params\n",
    ")\n",
    "\n",
    "print(\"Running sim...\")\n",
    "sim.run_sim(save_path)\n",
    "print(\"Sim finished.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "G-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
