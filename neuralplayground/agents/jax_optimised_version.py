# TODO: NOTE to self: This is a work in progress, it has not been tested to work, I think Jax is not a good way to implement in object oriented coding.
# I think if I want to implement it here I should use neuralplayground it would be in pytorch.

import argparse
import os
import shutil
from datetime import datetime
from typing import Union
from pathlib import Path
import haiku as hk
import jax
import jax.numpy as jnp
import numpy as np
import optax
import wandb
from neuralplayground.agents.agent_core import AgentCore

os.environ["KMP_DUPLICATE_LIB_OK"] = "True"
from neuralplayground.agents.domine_2023_extras.class_Graph_generation import (
    sample_padded_grid_batch_shortest_path,
)
from neuralplayground.agents.domine_2023_extras.class_grid_run_config import GridConfig
from neuralplayground.agents.domine_2023_extras.class_models import get_forward_function
from neuralplayground.agents.domine_2023_extras.class_plotting_utils import (
    plot_graph_grid_activations,
    plot_input_target_output,
    plot_message_passing_layers,
    plot_xy,
)
from neuralplayground.agents.domine_2023_extras.class_utils import (
    rng_sequence_from_rng,
    set_device,
)
from sklearn.metrics import matthews_corrcoef, roc_auc_score
from flax import struct


class Domine2023(AgentCore, struct.PytreeNode):
    experiment_name: str = "smaller size generalisation graph with  no position feature"
    train_on_shortest_path: bool = True
    resample: bool = True
    wandb_on: bool = False
    seed: int = 41
    feature_position: bool = False
    weighted: bool = True
    num_hidden: int = 100
    num_layers: int = 2
    num_message_passing_steps: int = 3
    learning_rate: float = 0.001
    num_training_steps: int = 10
    batch_size: int = 4
    nx_min: int = 4
    nx_max: int = 7
    batch_size_test: int = 4
    nx_min_test: int = 4
    nx_max_test: int = 7
    obs_history: list = []

    def __init__(  # autogenerated
        self,
        # agent_name: str = "SR",
        experiment_name="smaller size generalisation graph with  no position feature",
        train_on_shortest_path: bool = True,
        resample: bool = True,
        wandb_on: bool = False,
        seed: int = 41,
        feature_position: bool = False,
        weighted: bool = True,
        num_hidden: int = 100,
        num_layers: int = 2,
        num_message_passing_steps: int = 3,
        learning_rate: float = 0.001,
        num_training_steps: int = 10,
        batch_size: int = 4,
        nx_min: int = 4,
        nx_max: int = 7,
        batch_size_test: int = 4,
        nx_min_test: int = 4,
        nx_max_test: int = 7,
        **mod_kwargs,
    ):
        self.obs_history = []
        self.grad_history = []
        self.train_on_shortest_path = train_on_shortest_path
        self.experiment_name = experiment_name
        self.train_on_shortest_path = train_on_shortest_path
        self.resample = resample
        self.wandb_on = wandb_on
        self.seed = seed

        self.feature_position = feature_position
        self.weighted = weighted

        self.num_hidden = num_hidden
        self.num_layers = num_layers
        self.num_message_passing_steps = num_message_passing_steps
        self.learning_rate = learning_rate
        self.num_training_steps = num_training_steps
        # cconfig.num_training_steps  # @param

        self.batch_size = batch_size
        self.nx_min = nx_min
        self.nx_max = nx_max

        # This can be tought of the brain making different rep of different  granularity
        # Could be explained during sleep
        self.batch_size_test = batch_size_test  # cconfig.batch_size_test
        self.nx_min_test = (
            nx_min_test  # cconfig.nx_min_test  # This is thought of the state density
        )
        self.nx_max_test = (
            nx_max_test  # config.nx_max_test  # This is thought of the state density
        )
        self.batch_size = batch_size  # c config.batch_size
        self.nx_min = nx_min  # c config.nx_min  # This is thought of the state density
        self.nx_max = nx_max  # c config.nx_max  # This is thought of the state density

        self.arena_x_limits = mod_kwargs[
            "arena_y_limits"
        ]  # cmod_kwargs["arena_x_limits"]
        self.arena_y_limits = mod_kwargs["arena_y_limits"]
        self.room_width = np.diff(self.arena_x_limits)[0]
        self.room_depth = np.diff(self.arena_y_limits)[0]
        self.agent_step_size = 0

        self.log_every = num_training_steps // 10
        if self.weighted:
            self.edege_lables = True
        else:
            self.edege_lables = False

        if self.wandb_on:
            dateTimeObj = datetime.now()
            wandb.init(
                project="graph-brain",
                entity="graph-brain",
                name="Grid_shortest_path" + dateTimeObj.strftime("%d%b_%H_%M"),
            )
            self.wandb_logs = {}
            save_path = wandb.run.dir
            os.mkdir(os.path.join(save_path, "results"))
            self.save_path = os.path.join(save_path, "results")

        else:
            dateTimeObj = datetime.now()
            save_path = os.path.join(Path(os.getcwd()).resolve(), "results")
            os.mkdir(
                os.path.join(
                    save_path, "Grid_shortest_path" + dateTimeObj.strftime("%d%b_%H_%M")
                )
            )
            self.save_path = os.path.join(
                os.path.join(
                    save_path, "Grid_shortest_path" + dateTimeObj.strftime("%d%b_%H_%M")
                )
            )

        self.reset()
        self.saving_run_parameters()

        rng = jax.random.PRNGKey(self.seed)
        self.rng_seq = rng_sequence_from_rng(rng)

        if self.train_on_shortest_path:
            self.graph, self.targets = sample_padded_grid_batch_shortest_path(
                rng,
                self.batch_size,
                self.feature_position,
                self.weighted,
                self.nx_min,
                self.nx_max,
            )
        else:
            self.graph, self.targets = sample_padded_grid_batch_shortest_path(
                rng,
                self.batch_size,
                self.feature_position,
                self.weighted,
                self.nx_min,
                self.nx_max,
            )

        forward = get_forward_function(
            self.num_hidden, self.num_layers, self.num_message_passing_steps
        )
        net_hk = hk.without_apply_rng(hk.transform(forward))
        params = net_hk.init(rng, self.graph)
        self.params = params
        optimizer = optax.adam(self.learning_rate)
        opt_state = optimizer.init(self.params)
        self.opt_state = opt_state

        def compute_loss(params, inputs, targets):
            # not jitted because it will get jitted in jax.value_and_grad
            outputs = net_hk.apply(params, inputs)
            return jnp.mean((outputs[0].nodes - targets) ** 2)  # using MSE

        self._compute_loss = jax.jit(compute_loss)

        def update_step(params, opt_state):
            loss, grads = jax.value_and_grad(compute_loss)(
                params, self.graph, self.targets
            )  # jits inside of value_and_grad
            updates, opt_state = optimizer.update(grads, opt_state, params)
            params = optax.apply_updates(params, updates)
            return params, opt_state, loss

        self._update_step = jax.jit(update_step)

        def evaluate(params, inputs, target):
            outputs = net_hk.apply(params, inputs)
            roc_auc = roc_auc_score(np.squeeze(target), np.squeeze(outputs[0].nodes))
            MCC = matthews_corrcoef(
                np.squeeze(target), round(np.squeeze(outputs[0].nodes))
            )
            return outputs, roc_auc, MCC

        self._evaluate = evaluate

    @jit
    def compute_loss(self, inputs, targets):
        forward = get_forward_function(
            self.num_hidden, self.num_layers, self.num_message_passing_steps
        )
        net_hk = hk.without_apply_rng(hk.transform(forward))
        outputs = net_hk.apply(self.params, inputs)
        return jnp.mean((outputs[0].nodes - targets) ** 2)  # using MSE

    def saving_run_parameters(self):
        path = os.path.join(self.save_path, "run.py")
        HERE = os.path.join(Path(os.getcwd()).resolve(), "domine_2023.py")
        shutil.copyfile(HERE, path)

        path = os.path.join(self.save_path, "class_Graph_generation.py")
        HERE = os.path.join(
            Path(os.getcwd()).resolve(), "domine_2023_extras/class_Graph_generation.py"
        )
        shutil.copyfile(HERE, path)

        path = os.path.join(self.save_path, "class_utils.py")
        HERE = os.path.join(
            Path(os.getcwd()).resolve(), "domine_2023_extras/class_utils.py"
        )
        shutil.copyfile(HERE, path)

        path = os.path.join(self.save_path, "class_plotting_utils.py")
        HERE = os.path.join(
            Path(os.getcwd()).resolve(), "domine_2023_extras/class_plotting_utils.py"
        )
        shutil.copyfile(HERE, path)

        path = os.path.join(self.save_path, "class_config_run.yaml")
        HERE = os.path.join(
            Path(os.getcwd()).resolve(), "domine_2023_extras/class_config.yaml"
        )
        shutil.copyfile(HERE, path)

    def set_obs_history(self, obs_history):
        new_self = Domine2023(
            self.seed, self.num_hidden, self.num_layers, ..., obs_history=obs_history
        )
        return new_self

    def reset_obs_history(self, a=1):
        new_self = Domine2023(
            self.seed, self.num_hidden, self.num_layers, ..., obs_history=[]
        )
        return new_self

    def reset(self, a=1):
        self.obs_history = []  # Initialize observation history to update weights later
        self.grad_history = []
        self.global_steps = 0
        self.losses = []
        self.losses_test = []
        self.roc_aucs_train = []
        self.MCCs_train = []
        self.MCCs_test = []
        self.roc_aucs_test = []
        return

    def update(self):
        rng = next(self.rng_seq)
        graph_test, target_test = sample_padded_grid_batch_shortest_path(
            rng,
            self.batch_size_test,
            self.feature_position,
            self.weighted,
            self.nx_min_test,
            self.nx_max_test,
        )
        rng = next(self.rng_seq)
        # Sample a new batch of graph every itterations
        if self.resample:
            if self.train_on_shortest_path:
                self.graph, self.targets = sample_padded_grid_batch_shortest_path(
                    rng,
                    self.batch_size,
                    self.feature_position,
                    self.weighted,
                    self.nx_min,
                    self.nx_max,
                )
            else:
                self.graph, self.targets = sample_padded_grid_batch_shortest_path(
                    rng,
                    self.batch_size,
                    self.feature_position,
                    self.weighted,
                    self.nx_min,
                    self.nx_max,
                )
                self.targets = self.graph.nodes
        # Train

        self.params, self.opt_state, loss = self._update_step(
            self.params, self.opt_state
        )
        self.losses.append(loss)
        outputs_train, roc_auc_train, MCC_train = self._evaluate(
            self.params, self.graph, self.targets
        )
        self.roc_aucs_train.append(roc_auc_train)
        self.MCCs_train.append(MCC_train)  # Matthews correlation coefficient

        # Test # model should basically learn to do nothing from this
        loss_test = self._compute_loss(self.params, graph_test, target_test)
        self.losses_test.append(loss_test)
        outputs_test, roc_auc_test, MCC_test = self._evaluate(
            self.params, graph_test, target_test
        )
        self.roc_aucs_test.append(roc_auc_test)
        self.MCCs_test.append(MCC_test)

        # Log
        wandb_logs = {
            "loss": loss,
            "losses_test": loss_test,
            "roc_auc_test": roc_auc_test,
            "roc_auc": roc_auc_train,
        }
        if self.wandb_on:
            wandb.log(wandb_logs)
        self.global_steps = self.global_steps + 1
        if self.global_steps % self.log_every == 0:
            print(
                f"Training step {self.global_steps}: loss = {loss} , loss_test = {loss_test}, roc_auc_test = {roc_auc_test}, roc_auc_train = {roc_auc_train}"
            )
        return

    def print_and_plot(self):
        new_self = self.replace(obs_history=[])
        # EVALUATE
        rng = next(self.rng_seq)
        graph_test, target_test = sample_padded_grid_batch_shortest_path(
            rng,
            self.batch_size_test,
            self.feature_position,
            self.weighted,
            self.nx_min_test,
            self.nx_max_test,
        )
        # graph_test= self.graph
        # target_test = self.targets

        outputs, roc_auc, MCC = self._evaluate(self.params, graph_test, target_test)

        print("roc_auc_score")
        print(roc_auc)
        print("MCC")
        print(MCC)

        # SAVE PARAMETER (NOT WE SAVE THE FILES SO IT SHOULD BE THERE AS WELL )
        if self.wandb_on:
            with open("readme.txt", "w") as f:
                f.write("readme")
            with open(os.path.join(self.save_path, "Constant.txt"), "w") as outfile:
                outfile.write(
                    "num_message_passing_steps"
                    + str(self.num_message_passing_steps)
                    + "\n"
                )
                outfile.write("Learning_rate:" + str(self.learning_rate) + "\n")
                outfile.write("num_training_steps:" + str(self.num_training_steps))
                outfile.write("roc_auc" + str(roc_auc))
                outfile.write("MCC" + str(MCC))

        # PLOTTING THE LOSS and AUC ROC
        plot_xy(self.losses, os.path.join(self.save_path, "Losses.pdf"), "Losses")
        plot_xy(
            self.losses_test,
            os.path.join(self.save_path, "Losses_test.pdf"),
            "Losses_test",
        )
        plot_xy(
            self.roc_aucs_test,
            os.path.join(self.save_path, "auc_roc_test.pdf"),
            "auc_roc_test",
        )
        plot_xy(
            self.roc_aucs_train,
            os.path.join(self.save_path, "auc_roc_train.pdf"),
            "auc_roc_train",
        )
        plot_xy(
            self.MCCs_train, os.path.join(self.save_path, "MCC_train.pdf"), "MCC_train"
        )
        plot_xy(
            self.MCCs_test, os.path.join(self.save_path, "MCC_test.pdf"), "MCC_test"
        )

        # PLOTTING ACTIVATION OF THE FIRST 2 GRAPH OF THE BATCH
        plot_input_target_output(
            list(graph_test.nodes.sum(-1)),
            target_test.sum(-1),
            outputs[0].nodes.tolist(),
            graph_test,
            4,
            self.edege_lables,
            os.path.join(self.save_path, "in_out_targ.pdf"),
        )
        plot_message_passing_layers(
            list(graph_test.nodes.sum(-1)),
            outputs[1],
            target_test.sum(-1),
            outputs[0].nodes.tolist(),
            graph_test,
            3,
            self.num_message_passing_steps,
            self.edege_lables,
            os.path.join(self.save_path, "message_passing_graph.pdf"),
        )
        # plot_message_passing_layers_units(outputs[1], target_test.sum(-1), outputs[0].nodes.tolist(),graph_test,config.num_hidden,config.num_message_passing_steps,edege_lables,os.path.join(save_path, 'message_passing_hidden_unit.pdf'))

        # Plot each seperatly
        plot_graph_grid_activations(
            outputs[0].nodes.tolist(),
            graph_test,
            os.path.join(self.save_path, "outputs.pdf"),
            "Predicted Node Assignments with GCN",
            self.edege_lables,
        )
        plot_graph_grid_activations(
            list(graph_test.nodes.sum(-1)),
            graph_test,
            os.path.join(self.save_path, "Inputs.pdf"),
            "Inputs node assigments",
            self.edege_lables,
        )
        plot_graph_grid_activations(
            target_test.sum(-1),
            graph_test,
            os.path.join(self.save_path, "Target.pdf"),
            "Target",
            self.edege_lables,
        )

        graph_test = self.graph
        target_test = self.targets

        outputs, roc_auc, MCC = self._evaluate(self.params, graph_test, target_test)
        # PLOTTING ACTIVATION OF THE FIRST 2 GRAPH OF THE BATCH
        plot_input_target_output(
            list(graph_test.nodes.sum(-1)),
            target_test.sum(-1),
            outputs[0].nodes.tolist(),
            graph_test,
            4,
            self.edege_lables,
            os.path.join(self.save_path, "in_out_targ.pdf"),
        )
        plot_message_passing_layers(
            list(graph_test.nodes.sum(-1)),
            outputs[1],
            target_test.sum(-1),
            outputs[0].nodes.tolist(),
            graph_test,
            3,
            self.num_message_passing_steps,
            self.edege_lables,
            os.path.join(self.save_path, "message_passing_graph.pdf"),
        )
        # plot_message_passing_layers_units(outputs[1], target_test.sum(-1), outputs[0].nodes.tolist(),graph_test,config.num_hidden,config.num_message_passing_steps,edege_lables,os.path.join(save_path, 'message_passing_hidden_unit.pdf'))

        # Plot each seperatly
        plot_graph_grid_activations(
            outputs[0].nodes.tolist(),
            graph_test,
            os.path.join(self.save_path, "outputs.pdf"),
            "Predicted Node Assignments with GCN",
            self.edege_lables,
        )
        plot_graph_grid_activations(
            list(graph_test.nodes.sum(-1)),
            graph_test,
            os.path.join(self.save_path, "Inputs.pdf"),
            "Inputs node assigments",
            self.edege_lables,
        )
        plot_graph_grid_activations(
            target_test.sum(-1),
            graph_test,
            os.path.join(self.save_path, "Target.pdf"),
            "Target",
            self.edege_lables,
        )

        #  plot_graph_grid_activations(
        #     outputs[0].nodes.tolist(),
        #    graph_test,
        #   os.path.join(self.save_path, "outputs_2.pdf"),
        #  "Predicted Node Assignments with GCN",
        # self.edege_lables,
        # 2,
        # )
        # plot_graph_grid_activations(
        #   list(graph_test.nodes.sum(-1)),
        #  graph_test,
        # os.path.join(self.save_path, "Inputs_2.pdf"),
        # "Inputs node assigments",
        # self.edege_lables,
        # 2,
        # )
        #
        #
        print("End")

        # plot_graph_grid_activations(
        #
        #            target_test.sum(-1), graph_test, os.path.join(self.save_path, "Target_2.pdf"), "Target", self.edege_lables, 2
        # )
        #        return


if __name__ == "__main__":
    from neuralplayground.arenas import Simple2D

    # @title Graph net functions
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--config_path",
        metavar="-C",
        default="domine_2023_extras/class_config.yaml",
        help="path to base configuration file.",
    )

    args = parser.parse_args()
    set_device()
    config_class = GridConfig
    config = config_class(args.config_path)
    time_step_size = 0.1  # seg
    agent_step_size = 3

    # Init environment
    arena_x_limits = [-100, 100]
    arena_y_limits = [-100, 100]
    env = Simple2D(
        time_step_size=time_step_size,
        agent_step_size=agent_step_size,
        arena_x_limits=arena_x_limits,
        arena_y_limits=arena_y_limits,
    )

    agent = Domine2023(
        experiment_name=config.experiment_name,
        train_on_shortest_path=config.train_on_shortest_path,
        resample=config.resample,  # @param
        wandb_on=config.wandb_on,
        seed=config.seed,
        feature_position=config.feature_position,
        weighted=config.weighted,
        num_hidden=config.num_hidden,  # @param
        num_layers=config.num_layers,  # @param
        num_message_passing_steps=config.num_message_passing_steps,  # @param
        learning_rate=config.learning_rate,  # @param
        num_training_steps=config.num_training_steps,  # @param
        batch_size=config.batch_size,
        nx_min=config.nx_min,
        nx_max=config.nx_max,
        batch_size_test=config.batch_size_test,
        nx_min_test=config.nx_min_test,
        nx_max_test=7,
        arena_y_limits=arena_y_limits,
        arena_x_limits=arena_x_limits,
    )

    for n in range(config.num_training_steps):
        agent.update()
    agent.print_and_plot()

# TODO: Run manadger (not possible for now), to get a seperated code we would juste need to change the paths and config this would mean get rid of the comfig
# The other alternative is to see that we have multiple env that we resample every time
# TODO: Make juste an env type (so that is accomodates for not only 2 d env// different transmats)
# TODO: Make The plotting in the general plotting utilse
if __name__ == "__main__":
    x = Domine2023()
    x = x.replace(obs_history=[1, 2], num_hidden=2)
    x.num_hidden = 5

    x.update()
